{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNbUYHGBxa2repoNb1txji6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aneeshkrishna4739/CompanyDocAnalyser_Using_Rag/blob/main/CodebaseRag.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Install Necessary Libraries**\n"
      ],
      "metadata": {
        "id": "ONAQHsJFXeHq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GFytbz7mQR3v",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3673db65-96b7-40bf-a510-8cca07956d53"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pygithub in /usr/local/lib/python3.11/dist-packages (2.6.1)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.20)\n",
            "Requirement already satisfied: langchain-community in /usr/local/lib/python3.11/dist-packages (0.3.19)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (1.61.1)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.11/dist-packages (0.9.0)\n",
            "Requirement already satisfied: pinecone-client in /usr/local/lib/python3.11/dist-packages (6.0.0)\n",
            "Requirement already satisfied: langchain_pinecone in /usr/local/lib/python3.11/dist-packages (0.2.3)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (3.4.1)\n",
            "Requirement already satisfied: pynacl>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from pygithub) (1.5.0)\n",
            "Requirement already satisfied: requests>=2.14.0 in /usr/local/lib/python3.11/dist-packages (from pygithub) (2.32.3)\n",
            "Requirement already satisfied: pyjwt>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from pyjwt[crypto]>=2.4.0->pygithub) (2.10.1)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from pygithub) (4.12.2)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from pygithub) (2.3.0)\n",
            "Requirement already satisfied: Deprecated in /usr/local/lib/python3.11/dist-packages (from pygithub) (1.2.18)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.41 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.44)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.6 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.6)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.13)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.10.6)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.39)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (3.10.11)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (9.0.0)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.8.1)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.4.0)\n",
            "Requirement already satisfied: numpy<3,>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (1.26.4)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.9.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2024.11.6)\n",
            "Requirement already satisfied: certifi>=2019.11.17 in /usr/local/lib/python3.11/dist-packages (from pinecone-client) (2025.1.31)\n",
            "Requirement already satisfied: pinecone-plugin-interface<0.0.8,>=0.0.7 in /usr/local/lib/python3.11/dist-packages (from pinecone-client) (0.0.7)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from pinecone-client) (2.8.2)\n",
            "Requirement already satisfied: pinecone<6.0.0,>=5.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain_pinecone) (5.4.2)\n",
            "Requirement already satisfied: langchain-tests<1.0.0,>=0.3.7 in /usr/local/lib/python3.11/dist-packages (from langchain_pinecone) (0.3.14)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.48.3)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.6.0+cu124)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.14.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (0.28.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.1.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.18.3)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.17.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.41->langchain) (1.33)\n",
            "Requirement already satisfied: pytest<9,>=7 in /usr/local/lib/python3.11/dist-packages (from langchain-tests<1.0.0,>=0.3.7->langchain_pinecone) (8.3.5)\n",
            "Requirement already satisfied: pytest-asyncio<1,>=0.20 in /usr/local/lib/python3.11/dist-packages (from langchain-tests<1.0.0,>=0.3.7->langchain_pinecone) (0.25.3)\n",
            "Requirement already satisfied: syrupy<5,>=4 in /usr/local/lib/python3.11/dist-packages (from langchain-tests<1.0.0,>=0.3.7->langchain_pinecone) (4.9.0)\n",
            "Requirement already satisfied: pytest-socket<1,>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from langchain-tests<1.0.0,>=0.3.7->langchain_pinecone) (0.7.0)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.15)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
            "Requirement already satisfied: pinecone-plugin-inference<4.0.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from pinecone<6.0.0,>=5.4.0->langchain_pinecone) (3.1.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.27.2)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (1.0.1)\n",
            "Requirement already satisfied: cryptography>=3.4.0 in /usr/local/lib/python3.11/dist-packages (from pyjwt[crypto]>=2.4.0->pygithub) (43.0.3)\n",
            "Requirement already satisfied: cffi>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from pynacl>=1.4.0->pygithub) (1.17.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.5.3->pinecone-client) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.14.0->pygithub) (3.4.1)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.11/dist-packages (from Deprecated->pygithub) (1.17.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.4.1->pynacl>=1.4.0->pygithub) (2.22)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.41->langchain) (3.0.0)\n",
            "Requirement already satisfied: iniconfig in /usr/local/lib/python3.11/dist-packages (from pytest<9,>=7->langchain-tests<1.0.0,>=0.3.7->langchain_pinecone) (2.0.0)\n",
            "Requirement already satisfied: pluggy<2,>=1.5 in /usr/local/lib/python3.11/dist-packages (from pytest<9,>=7->langchain-tests<1.0.0,>=0.3.7->langchain_pinecone) (1.5.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from yarl<2.0,>=1.12.0->aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "! pip install pygithub langchain langchain-community openai tiktoken pinecone-client langchain_pinecone sentence-transformers\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from langchain_pinecone import PineconeVectorStore\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from google.colab import userdata\n",
        "from pinecone import Pinecone\n",
        "import os\n",
        "import tempfile\n",
        "from github import Github, Repository\n",
        "from git import Repo\n",
        "from openai import OpenAI\n",
        "from pathlib import Path\n",
        "from langchain.schema import Document\n",
        "from pinecone import Pinecone"
      ],
      "metadata": {
        "id": "7Xx2kn-EXaRb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 547
        },
        "outputId": "9ff5cf4f-e20e-4d70-9285-e21a8d93d258"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Failed to import transformers.models.auto.modeling_auto because of the following error (look up to see its traceback):\nFailed to import transformers.generation.utils because of the following error (look up to see its traceback):\nNo module named 'numpy.char'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m_get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1816\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1817\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmodule_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1818\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbeam_search\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBeamScorer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBeamSearchScorer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mConstrainedBeamSearchScorer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m from .candidate_generator import (\n\u001b[0m\u001b[1;32m     54\u001b[0m     \u001b[0mAssistedCandidateGenerator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/generation/candidate_generator.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mis_sklearn_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mroc_curve\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     72\u001b[0m )\n\u001b[0;32m---> 73\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mclone\u001b[0m  \u001b[0;31m# noqa: E402\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_show_versions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mshow_versions\u001b[0m  \u001b[0;31m# noqa: E402\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInconsistentVersionWarning\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_estimator_html_repr\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_HTMLDocumentationLinkMixin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mestimator_html_repr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_metadata_requests\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_MetadataRequester\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_routing_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_bunch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBunch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_chunking\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgen_batches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgen_even_slices\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_estimator_html_repr\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mestimator_html_repr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/_chunking.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_config\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_param_validation\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInterval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_params\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/_param_validation.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcsr_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0missparse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/scipy/sparse/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    292\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 293\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_base\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    294\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_csr\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/scipy/sparse/_base.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m from ._sputils import (asmatrix, check_reshape_kwargs, check_shape,\n\u001b[0m\u001b[1;32m      6\u001b[0m                        \u001b[0mget_sum_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0misdense\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0misscalarlike\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/scipy/sparse/_sputils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_util\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnp_long\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp_ulong\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/scipy/_lib/_util.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_array_api\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0marray_namespace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_numpy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mxp_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/scipy/_lib/_array_api.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0marray_api_compat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m from scipy._lib.array_api_compat import (\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0mis_array_api_obj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/scipy/_lib/array_api_compat/numpy/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;31m# noqa: F403\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# from numpy import * doesn't overwrite these builtin names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numpy/__init__.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(attr)\u001b[0m\n\u001b[1;32m    366\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mAssertionError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 367\u001b[0;31m             msg = (\"The current Numpy installation ({!r}) fails to \"\n\u001b[0m\u001b[1;32m    368\u001b[0m                    \u001b[0;34m\"pass simple sanity checks. This can be caused for example \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'numpy.char'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m_get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1816\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1817\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmodule_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1818\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/auto/modeling_auto.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m from .auto_factory import (\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0m_BaseAutoBackboneClass\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mis_torch_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0mgeneration\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGenerationMixin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_handle_fromlist\u001b[0;34m(module, fromlist, import_, recursive)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1804\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1805\u001b[0;31m             \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1806\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m_get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1818\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1819\u001b[0;31m             raise RuntimeError(\n\u001b[0m\u001b[1;32m   1820\u001b[0m                 \u001b[0;34mf\"Failed to import {self.__name__}.{module_name} because of the following error (look up to see its\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Failed to import transformers.generation.utils because of the following error (look up to see its traceback):\nNo module named 'numpy.char'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-feec64e439e4>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msentence_transformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSentenceTransformer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpairwise\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcosine_similarity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain_pinecone\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPineconeVectorStore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mOpenAIEmbeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain_community\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mHuggingFaceEmbeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sentence_transformers/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mexport_static_quantized_openvino_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m )\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msentence_transformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_encoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrossEncoder\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCrossEncoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msentence_transformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mParallelSentencesDataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSentencesDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msentence_transformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLoggingHandler\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLoggingHandler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sentence_transformers/cross_encoder/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mannotations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mCrossEncoder\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCrossEncoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0m__all__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"CrossEncoder\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sentence_transformers/cross_encoder/CrossEncoder.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautonotebook\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrange\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAutoConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAutoModelForSequenceClassification\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_torch_npu_available\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenization_utils_base\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBatchEncoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPushToHubMixin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_handle_fromlist\u001b[0;34m(module, fromlist, import_, recursive)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1804\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1805\u001b[0m             \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1806\u001b[0;31m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1807\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1808\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1803\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPlaceholder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1804\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1805\u001b[0;31m             \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1806\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1807\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m_get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1817\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmodule_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1818\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1819\u001b[0;31m             raise RuntimeError(\n\u001b[0m\u001b[1;32m   1820\u001b[0m                 \u001b[0;34mf\"Failed to import {self.__name__}.{module_name} because of the following error (look up to see its\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1821\u001b[0m                 \u001b[0;34mf\" traceback):\\n{e}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Failed to import transformers.models.auto.modeling_auto because of the following error (look up to see its traceback):\nFailed to import transformers.generation.utils because of the following error (look up to see its traceback):\nNo module named 'numpy.char'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Clone a GitHub Repo locally**"
      ],
      "metadata": {
        "id": "_JG_jWOjYFjl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "github_repo = \"https://github.com/aneeshkrishna4739/InSightBot\"\n",
        "github_repo.split(\"/\")[-1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "NqwTO_fXYIYE",
        "outputId": "3012d676-9a50-4e12-8b45-dbe0683390c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'InSightBot'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def clone_repository(repo_url):\n",
        "    \"\"\"Clones a GitHub repository to a temporary directory.\n",
        "\n",
        "    Args:\n",
        "        repo_url: The URL of the GitHub repository.\n",
        "\n",
        "    Returns:\n",
        "        The path to the cloned repository.\n",
        "    \"\"\"\n",
        "\n",
        "    repo_name = github_repo.split(\"/\")[-1]\n",
        "    repo_path = f\"/content/{repo_name}\"\n",
        "    Repo.clone_from(repo_url, str(repo_path))\n",
        "    return repo_path"
      ],
      "metadata": {
        "id": "a1qjNpZEYMbm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path = clone_repository(github_repo)\n",
        "path"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "ZbjZbx9FYNWs",
        "outputId": "87225947-717b-4c29-bafd-8daf7c098672"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/InSightBot'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "QujRLc7qYchK",
        "outputId": "d82fe25c-be3d-450f-fbe3-e64fa3e0d463"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/SecureAgent'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Define which types of files to parse and which files / folders to ignore**"
      ],
      "metadata": {
        "id": "Aec7pnx1ZJe0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SUPPORTED_EXTENSIONS = {'.py', '.js', '.tsx', '.jsx', '.ipynb', '.java',\n",
        "                         '.cpp', '.ts', '.go', '.rs', '.vue', '.swift', '.c', '.h'}\n",
        "\n",
        "IGNORED_DIRS = {'node_modules', 'venv', 'env', 'dist', 'build', '.git',\n",
        "                '__pycache__', '.next', '.vscode', 'vendor'}"
      ],
      "metadata": {
        "id": "0l7qxxAGZK83"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_file_content(file_path, repo_path):\n",
        "    \"\"\"\n",
        "    Get content of a single file.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): Path to the file\n",
        "\n",
        "    Returns:\n",
        "        Optional[Dict[str, str]]: Dictionary with file name and content\n",
        "    \"\"\"\n",
        "    try:\n",
        "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            content = f.read()\n",
        "\n",
        "        rel_path = os.path.relpath(file_path, repo_path)\n",
        "\n",
        "        return {\n",
        "            \"name\": rel_path,\n",
        "            \"content\": content\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing file {file_path}: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "\n",
        "def get_main_files_content(repo_path: str):\n",
        "    \"\"\"\n",
        "    Get content of supported code files from the local repository.\n",
        "\n",
        "    Args:\n",
        "        repo_path: Path to the local repository\n",
        "\n",
        "    Returns:\n",
        "        List of dictionaries containing file names and contents\n",
        "    \"\"\"\n",
        "\n",
        "    files_content = []\n",
        "\n",
        "    try:\n",
        "\n",
        "        for root, _, files in os.walk(repo_path):\n",
        "            if any(ignored_dir in root for ignored_dir in IGNORED_DIRS):\n",
        "                continue\n",
        "\n",
        "            for file in files:\n",
        "                file_path = os.path.join(root, file)\n",
        "                if os.path.splitext(file)[1] in SUPPORTED_EXTENSIONS:\n",
        "                    file_content = get_file_content(file_path, repo_path)\n",
        "\n",
        "                    if file_content:\n",
        "                        files_content.append(file_content)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "\n",
        "    return files_content"
      ],
      "metadata": {
        "id": "uff8dPFpZPrU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "files_content = get_main_files_content(path)\n",
        "files_content"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LB0GCn3OZRsa",
        "outputId": "755a2254-4e3f-4a74-ffd7-664026e43461"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'name': 'app.py',\n",
              "  'content': 'from flask import Flask, render_template, request, jsonify\\nfrom groq_utils import GroqClass\\nfrom preprocessing import DocumentProcessor\\nfrom transcriber import YouTubeTranscriber\\nfrom langchain_community.embeddings import HuggingFaceEmbeddings\\nimport os\\nfrom dotenv import load_dotenv\\napp = Flask(__name__)\\n# Load environment variables from .env file\\nload_dotenv()\\n\\nindex_name = \"insightbot\"\\nnamespace = \"transcripts\"\\n\\n# Initialize embeddings and shared classes\\nembeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\\ngroq_client = GroqClass()  # Initialize GroqClass globally to be reused\\ndocument_processor = DocumentProcessor(index_name=index_name,namespace=namespace)  # Initialize DocumentProcessor globpally\\n# GLOBAL\\ndocument_dir_path = \"resources/documents\"\\ntranscript_dir_path = \"resources/transcripts\"\\n\\n@app.route(\\'/\\')\\ndef home():\\n    return render_template(\\'index.html\\')\\n\\n@app.route(\\'/chatbot\\')\\ndef chatbot():\\n    return render_template(\\'chatbot.html\\')\\n\\n@app.route(\\'/submit_media\\', methods=[\\'POST\\'])\\ndef submit_media():\\n    try:\\n        groq_client.reset_memory()\\n        document_processor.delete_files_in_directory(transcript_dir_path)\\n        document_processor.delete_files_in_directory(document_dir_path)\\n\\n        # Get YouTube links and documents from the request\\n        youtube_links = request.form.getlist(\\'youtube_links[]\\')  # List of YouTube links\\n        files = request.files.getlist(\\'documents[]\\')  # List of uploaded files\\n\\n        # Validate input\\n        if not youtube_links and not files:\\n            return jsonify({\"error\": \"At least one YouTube link or document is required\"}), 400\\n\\n        all_documents = []\\n\\n        # Process each YouTube link\\n        if youtube_links:\\n            print(\"=== Processing YouTube Links ===\")\\n            for youtube_link in youtube_links:\\n                if youtube_link:\\n                    print(f\"Processing YouTube Link: {youtube_link}\")\\n                    transcriber = YouTubeTranscriber(youtube_link, output_dir = transcript_dir_path)\\n                    transcriber.transcribe()\\n                    print(f\"Transcription complete for: {youtube_link}\")\\n            documents = document_processor.process_directory(transcript_dir_path)\\n            all_documents.extend(documents)\\n\\n        # Process uploaded documents\\n        if files:\\n            print(\"=== Processing Uploaded Documents ===\")\\n            for file in files:\\n                file_path = os.path.join(document_dir_path, file.filename)\\n                file.save(file_path)\\n                print(f\"Saved file: {file_path}\")\\n            uploaded_documents = document_processor.process_directory(document_dir_path)\\n            all_documents.extend(uploaded_documents)\\n\\n        if not all_documents:\\n            return jsonify({\"error\": \"No valid documents to process\"}), 400\\n\\n        # Prepare and chunk data\\n        document_data = document_processor.prepare_data(all_documents)\\n        documents_chunks = document_processor.chunk_data(document_data)\\n\\n        vectorstore_from_documents = document_processor.upsert_vectorstore_to_pinecone(documents_chunks, embeddings, index_name, namespace)\\n        print(\\'=== Upsert to Pinecone done ===\\', vectorstore_from_documents)\\n\\n        return jsonify({\"message\": \"Media processed successfully!\"})\\n\\n    except Exception as e:\\n        print(f\"Error: {str(e)}\")\\n        return jsonify({\"error\": \"Failed to process media\", \"details\": str(e)}), 500\\n\\n@app.route(\\'/submit_youtube_link\\', methods=[\\'POST\\'])\\n\\n@app.route(\\'/ask_question\\', methods=[\\'POST\\'])\\ndef ask_question():\\n    question = request.form.get(\\'question\\')\\n    # Get the response from the model\\n    pinecone_index = groq_client.initialize_pinecone(index_name)\\n    answer=groq_client.perform_rag(pinecone_index, namespace, question)\\n    return jsonify({\"question\": question, \"answer\": answer})\\n\\n# Simulating model response for now\\ndef get_model_response(question):\\n    # Replace with actual logic to handle YouTube video and query\\n    return f\"Answer to your question \\'{question}\\'\"\\n\\n\\n# if __name__ == \"__main__\":\\n#     app.run(debug=False)\\nif __name__==\"__main__\":\\n    app.run(host=os.getenv(\\'IP\\', \\'0.0.0.0\\'), \\n            port=int(os.getenv(\\'PORT\\', 4444)))'},\n",
              " {'name': 'static/script.js',\n",
              "  'content': \"\\n// Function to handle question submission and display chat\\nfunction askQuestion() {\\n    const question = document.getElementById('question-input').value;\\n\\n    if (!question) {\\n        alert('Please ask a question');\\n        return;\\n    }\\n\\n    const chatBox = document.getElementById('chat-box');\\n\\n    // Display the question in chat\\n    const questionElement = document.createElement('div');\\n    questionElement.className = 'chat-message question';\\n    questionElement.textContent = question;\\n    chatBox.appendChild(questionElement);\\n\\n    // Clear the input\\n    document.getElementById('question-input').value = '';\\n\\n    // Get the model response\\n    fetch('/ask_question', {\\n        method: 'POST',\\n        headers: {\\n            'Content-Type': 'application/x-www-form-urlencoded'\\n        },\\n        body: `question=${question}`\\n    })\\n    .then(response => response.json())\\n    .then(data => {\\n        // Display the model's answer in chat\\n        const answerElement = document.createElement('div');\\n        answerElement.className = 'chat-message answer';\\n        answerElement.textContent = data.answer;\\n        chatBox.appendChild(answerElement);\\n\\n        // Scroll to the bottom of the chat\\n        chatBox.scrollTop = chatBox.scrollHeight;\\n    })\\n    .catch(error => console.error('Error:', error));\\n}\\n\\n// Function to clear chat \\nfunction clearChat() {\\n    // Clear chat messages\\n    document.getElementById('chat-box').innerHTML = '';\\n}\\n\\n// Store selected media\\nconst mediaItems = [];\\n\\n// Add YouTube Link\\nfunction addYoutubeLink() {\\n    const link = document.getElementById('youtube-link').value;\\n    if (link) {\\n        mediaItems.push({ type: 'YouTube', value: link });\\n        updateMediaList();\\n        document.getElementById('youtube-link').value = '';\\n    } else {\\n        alert('Please enter a valid YouTube link.');\\n    }\\n}\\n\\n// Add Uploaded Documents\\nfunction addDocuments() {\\n    const files = document.getElementById('upload-docs').files;\\n    for (let i = 0; i < files.length; i++) {\\n        // Push the entire file object, not just the name\\n        mediaItems.push({ type: 'Document', value: files[i] });\\n    }\\n    updateMediaList();\\n}\\n\\n// Update the Media List Display\\nfunction updateMediaList() {\\n    const list = document.getElementById('media-items');\\n    list.innerHTML = '';\\n    mediaItems.forEach(item => {\\n        const li = document.createElement('li');\\n        li.textContent = `${item.type}: ${\\n            item.type === 'Document' ? item.value.name : item.value\\n        }`;\\n        list.appendChild(li);\\n    });\\n}\\n\\n// Submit and process all media\\nfunction submitMedia() {\\n    const formData = new FormData();\\n\\n    // Add YouTube Links and Documents to FormData\\n    mediaItems.forEach(item => {\\n        if (item.type === 'YouTube') {\\n            formData.append('youtube_links[]', item.value);\\n        } else if (item.type === 'Document') {\\n            // Use the actual file object\\n            formData.append('documents[]', item.value);\\n        }\\n    });\\n\\n    // Debug: Log form data entries\\n    for (const [key, value] of formData.entries()) {\\n        console.log(`${key}:`, value);\\n    }\\n\\n    // Send FormData to the server\\n    return fetch('/submit_media', {\\n        method: 'POST',\\n        body: formData,\\n    });\\n}\\n\\n// Navigate to Chatbot Page\\nasync function goToChatbot() {\\n    if (mediaItems.length > 0) {\\n        try {\\n            // Call submitMedia and wait for the server response\\n            const response = await submitMedia();\\n            \\n            if (response.ok) {\\n                // Redirect to the chatbot page if submission is successful\\n                window.location.href = '/chatbot';\\n            } else {\\n                // Handle errors from the server\\n                const errorData = await response.json();\\n                alert(`Error: ${errorData.error || 'Failed to submit media.'}`);\\n            }\\n        } catch (error) {\\n            console.error('Error submitting media:', error);\\n            alert('An error occurred while submitting media. Please try again.');\\n        }\\n    } else {\\n        alert('Please add at least one media item.');\\n    }\\n}\\n\"},\n",
              " {'name': 'preprocessing/Prepare.py',\n",
              "  'content': 'import os\\nimport time\\nfrom langchain_community.document_loaders import TextLoader\\nfrom langchain.schema import Document\\nfrom langchain_pinecone import PineconeVectorStore\\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\\nimport pinecone\\nfrom dotenv import load_dotenv\\nfrom pinecone import Pinecone\\nfrom pypdf import PdfReader\\nfrom docx import Document as DocxDocument\\nfrom langchain.schema import Document as LangchainDocument\\n\\n# Load environment variables\\nload_dotenv()\\n# os.environ[\\'PINECONE_API_KEY\\'] = os.environ.get(\\'PINECONE_API_KEY\\')\\n\\nclass DocumentProcessor:\\n    def __init__(self, index_name: str, namespace: str, chunk_size: int = 1000, chunk_overlap: int = 30):\\n        self.index_name = index_name\\n        self.namespace = namespace\\n        self.chunk_size = chunk_size\\n        self.chunk_overlap = chunk_overlap\\n\\n    def delete_files_in_directory(self, directory_path: str):\\n        \"\"\"\\n        Delete all files in a given directory.\\n        \"\"\"\\n        files = os.listdir(directory_path)\\n        print(\\'Files in dir:\\', files)\\n        for file in files:\\n            file_path = os.path.join(directory_path, file)\\n            if os.path.isfile(file_path):\\n                os.remove(file_path)\\n        print(\"All files deleted successfully.\")\\n\\n    def process_directory(self, directory_path: str):\\n        data = []\\n        \\n        for root, _, files in os.walk(directory_path):\\n            for file in files:\\n                file_path = os.path.join(root, file)\\n                print(f\"Processing file: {file_path}\")\\n                \\n                # Skip empty files\\n                if os.path.getsize(file_path) == 0:\\n                    print(f\"Warning: The file {file_path} is empty and will be skipped.\")\\n                    continue\\n                \\n                try:\\n                    # Process .txt files\\n                    if file.endswith(\".txt\"):\\n                        with open(file_path, \"r\", encoding=\"utf-8\") as f:\\n                            file_data = f.read()\\n                    \\n                    # Process .pdf files\\n                    elif file.endswith(\".pdf\"):\\n                        reader = PdfReader(file_path)\\n                        file_data = \"\"\\n                        for page in reader.pages:\\n                            file_data += page.extract_text()\\n                    \\n                    # Process .docx files\\n                    elif file.endswith(\".docx\"):\\n                        document = DocxDocument(file_path)\\n                        file_data = \"\\\\n\".join([para.text for para in document.paragraphs])\\n                    \\n                    else:\\n                        print(f\"Unsupported file format: {file}\")\\n                        continue\\n                    \\n                    # Ensure valid content was extracted\\n                    if not file_data.strip():\\n                        print(f\"Warning: No content extracted from {file_path}. Skipping.\")\\n                        continue\\n                    \\n                    # Append the processed data\\n                    data.append({\"File\": file_path, \"Data\": file_data})\\n                \\n                except Exception as e:\\n                    print(f\"Error processing file {file_path}: {e}\")\\n        return data\\n\\n    def prepare_data(self, documents):\\n        # Prepare the text for embedding\\n        document_data = []\\n        for document in documents:\\n            # Ensure \\'Data\\' exists and is non-empty\\n            if \\'Data\\' not in document or not document[\\'Data\\']:\\n                print(f\"Skipping document due to missing or empty \\'Data\\': {document}\")\\n                continue\\n            \\n            # Ensure the first element in \\'Data\\' is a Document object\\n            if not isinstance(document[\\'Data\\'][0], str):\\n                print(f\"Skipping document due to invalid data type in \\'Data\\': {document}\")\\n                continue\\n\\n            # Extract metadata and content\\n            document_source = document[\\'File\\']\\n            document_content = document[\\'Data\\']\\n\\n            file_name = document_source.split(\"/\")[-1]\\n            folder_names = document_source.split(\"/\")[2:-1] if \"/\" in document_source else []\\n\\n            doc = LangchainDocument(\\n                page_content=f\"<Source>\\\\n{document_source}\\\\n</Source>\\\\n\\\\n<Content>\\\\n{document_content}\\\\n</Content>\",\\n                metadata={\\n                    \"file_name\": file_name,\\n                    \"parent_folder\": folder_names[-1] if folder_names else \"\",\\n                    \"folder_names\": folder_names\\n                }\\n            )\\n            document_data.append(doc)\\n\\n        return document_data\\n\\n    def chunk_data(self, docs):\\n        \"\"\"\\n        Split documents into chunks based on the provided chunk size and overlap.\\n        \"\"\"\\n        textsplitter = RecursiveCharacterTextSplitter(chunk_size=self.chunk_size, chunk_overlap=self.chunk_overlap)\\n        docs = textsplitter.split_documents(docs)\\n        return docs\\n\\n    def upsert_vectorstore_to_pinecone(self, document_data, embeddings, index_name, namespace):\\n        # # Initialize Pinecone connection with the new API structure\\n        pc = pinecone.Pinecone(api_key=os.environ.get(\\'PINECONE_API_KEY\\'))\\n\\n        # Check if the namespace exists in the index\\n        index = pc.Index(self.index_name)\\n\\n        # Check if the namespace exists by listing the namespaces (or by trying to query)\\n        namespaces = index.describe_index_stats().get(\\'namespaces\\', [])\\n        max_retries = 5\\n        wait_time = 2000\\n        if self.namespace in namespaces:\\n            print(f\"Namespace \\'{self.namespace}\\' found. Deleting vector data...\")\\n            index.delete(namespace=self.namespace, delete_all=True)  # Initiates deletion\\n\\n            # Polling to ensure deletion completes\\n            for attempt in range(max_retries):\\n                namespaces = index.describe_index_stats().get(\\'namespaces\\', [])\\n                if self.namespace not in namespaces:\\n                    print(f\"Namespace \\'{self.namespace}\\' deletion confirmed.\")\\n                    break\\n                time.sleep(wait_time)  # Wait before re-checking\\n            else:\\n                raise RuntimeError(f\"Failed to delete namespace \\'{self.namespace}\\' after {max_retries} retries.\")\\n\\n        else:\\n            print(f\"Namespace \\'{self.namespace}\\' does not exist. Proceeding with upsert.\")\\n\\n        # Create or replace the vector store\\n        vectorstore_from_documents = PineconeVectorStore.from_documents(\\n            document_data,\\n            embeddings,\\n            index_name=self.index_name,\\n            namespace=self.namespace\\n        )\\n        print(f\"Vector store type: {type(vectorstore_from_documents)}\")\\n\\n        # Optionally, return the vector store if needed\\n        return vectorstore_from_documents\\n\\n    def initialize_pinecone(self):\\n        \"\"\"\\n        Initialize Pinecone with the provided index name.\\n        \"\"\"\\n        # Initialize Pinecone instance\\n        pc = Pinecone(api_key=os.environ.get(\\'PINECONE_API_KEY\\'))\\n\\n        # Check if the index exists\\n        if self.index_name not in [index.name for index in pc.list_indexes().indexes]:\\n            raise ValueError(f\"Index \\'{self.index_name}\\' does not exist. Please create it first.\")\\n\\n        # Connect to the specified index\\n        pinecone_index = pc.Index(self.index_name)\\n        return pinecone_index\\n'},\n",
              " {'name': 'preprocessing/__init__.py',\n",
              "  'content': 'from .Prepare import DocumentProcessor'},\n",
              " {'name': 'transcriber/__init__.py',\n",
              "  'content': 'from .youtube_transcriber import YouTubeTranscriber\\n'},\n",
              " {'name': 'transcriber/youtube_transcriber.py',\n",
              "  'content': 'import os\\nfrom pytube import Playlist\\nfrom youtube_transcript_api import YouTubeTranscriptApi\\nimport ssl\\nclass YouTubeTranscriber:\\n    def __init__(self, url, transcript_language=\\'en\\', output_dir=\\'resources/transcripts\\'):\\n        \"\"\"\\n        Initializes the YouTubeTranscriber with a URL (playlist or single video), transcript language, and output directory.\\n\\n        :param url: URL of the YouTube playlist or single video\\n        :param transcript_language: Preferred language for transcripts (default is \\'en\\' for English)\\n        :param output_dir: Directory where transcripts will be saved\\n        \"\"\"\\n        self.url = url\\n        self.transcript_language = transcript_language\\n        self.output_dir = output_dir\\n\\n        # Check if the URL is for a playlist or a single video\\n        if \\'playlist?list=\\' in url:\\n            self.is_playlist = True\\n            self.playlist = []\\n            # Disable SSL verification\\n            ssl._create_default_https_context = ssl._create_unverified_context\\n            playlist_urls = Playlist(url)\\n            for url in playlist_urls:\\n                self.playlist.append(url)\\n        elif \\'watch?v=\\' in url or \\'youtu.be/\\' in url:\\n            self.is_playlist = False\\n            if \\'watch?v=\\' in url:\\n                # Extract video ID from \"https://www.youtube.com/watch?v=\"\\n                self.video_id = url.split(\\'watch?v=\\')[1].split(\\'&\\')[0]\\n            elif \\'youtu.be/\\' in url:\\n                # Extract video ID from \"https://youtu.be/\"\\n                self.video_id = url.split(\\'youtu.be/\\')[1].split(\\'?\\')[0]\\n        else:\\n            raise ValueError(\"Invalid URL. Provide a valid YouTube playlist or video URL.\")\\n\\n        # Ensure output directory exists\\n        os.makedirs(self.output_dir, exist_ok=True)\\n        if self.is_playlist:\\n            print(f\"Found {len(self.playlist)} videos in the playlist.\")\\n\\n    def fetch_transcript(self, video_id):\\n        \"\"\"\\n        Fetches the transcript for a single video in the specified language.\\n\\n        :param video_id: YouTube video ID\\n        :return: Transcript as a list of dictionaries with \\'start\\' and \\'text\\' keys\\n        \"\"\"\\n        try:\\n            # Fetch transcript with the specified language\\n            transcript = YouTubeTranscriptApi.get_transcript(video_id, languages=[self.transcript_language])\\n            return transcript\\n        except Exception as e:\\n            print(f\"Could not retrieve transcript for video ID {video_id}: {e}\")\\n            return None\\n\\n    def save_transcript_to_file(self, video_id, transcript):\\n        \"\"\"\\n        Saves the transcript of a video to a text file.\\n\\n        :param video_id: YouTube video ID\\n        :param transcript: Transcript data to save\\n        \"\"\"\\n        file_path = os.path.join(self.output_dir, f\"{video_id}_transcript.txt\")\\n        with open(file_path, \"w\", encoding=\"utf-8\") as file:\\n            for line in transcript:\\n                file.write(f\"{line[\\'start\\']}: {line[\\'text\\']}\\\\n\")\\n        print(f\"Transcript saved for video ID: {video_id}\")\\n\\n    def transcribe_playlist(self):\\n        \"\"\"\\n        Processes each video in the playlist to fetch and save transcripts.\\n        \"\"\"\\n        for video_url in self.playlist:\\n            # Extract video ID from URL\\n            video_id = video_url.split(\\'=\\')[-1]\\n            if \\'watch?v=\\' in video_url:\\n                # Extract video ID from \"https://www.youtube.com/watch?v=\"\\n                video_id = video_url.split(\\'watch?v=\\')[1].split(\\'&\\')[0]\\n            elif \\'youtu.be/\\' in video_url:\\n                # Extract video ID from \"https://youtu.be/\"\\n                video_id = video_url.split(\\'youtu.be/\\')[1].split(\\'?\\')[0]\\n            else:\\n                raise ValueError(\"Invalid URL. Provide a valid YouTube playlist or video URL.\")\\n            # Fetch and save the transcript\\n            transcript = self.fetch_transcript(video_id)\\n            if transcript:\\n                self.save_transcript_to_file(video_id, transcript)\\n\\n    def transcribe_single_video(self):\\n        \"\"\"\\n        Fetches and saves the transcript for a single YouTube video.\\n        \"\"\"\\n        # Fetch and save the transcript\\n        transcript = self.fetch_transcript(self.video_id)\\n        if transcript:\\n            self.save_transcript_to_file(self.video_id, transcript)\\n\\n    def transcribe(self):\\n        \"\"\"\\n        Determines if the URL is a playlist or single video and processes accordingly.\\n        \"\"\"\\n        if self.is_playlist:\\n            self.transcribe_playlist()\\n        else:\\n            self.transcribe_single_video()'},\n",
              " {'name': 'groq_utils/GroqClass.py',\n",
              "  'content': 'from sentence_transformers import SentenceTransformer\\nfrom groq import Groq\\nimport numpy as np\\nfrom pinecone import Pinecone\\nfrom langchain_community.embeddings import HuggingFaceEmbeddings\\nfrom dotenv import load_dotenv\\nimport os\\n\\nclass GroqClass:\\n    def __init__(self, model_name=\"sentence-transformers/all-MiniLM-L6-v2\"):\\n        \"\"\"\\n        Initialize the class with necessary keys and model for embeddings.\\n        \"\"\"\\n        # Load environment variables from .env file\\n        load_dotenv()\\n\\n        # Set environment variables from .env\\n\\n        # self.pinecone_api_key = os.environ.get(\\'PINECONE_API_KEY\\')\\n        # self.groq_api_key = os.environ.get(\\'GROQ_API_KEY\\')\\n        self.model_name = model_name\\n\\n        # print(f\"GROQ_API_KEY: {self.groq_api_key}\")\\n        # print(f\"PINECONE_API_KEY: {self.pinecone_api_key}\")\\n        # Initialize the Groq client with the provided API key\\n        self.groq_client = Groq(api_key=os.environ.get(\\'GROQ_API_KEY\\'))\\n\\n        # Initialize embeddings\\n        self.embeddings = HuggingFaceEmbeddings(model_name=self.model_name)\\n\\n        # Initialize Pinecone client\\n        self.pinecone = Pinecone(api_key=os.environ.get(\\'PINECONE_API_KEY\\'))\\n\\n        self.conversation_history=[]\\n\\n    def get_huggingface_embeddings(self, text):\\n        \"\"\"\\n        Get the Hugging Face embeddings for a given text.\\n        \"\"\"\\n        model = SentenceTransformer(self.model_name)\\n        return model.encode(text)\\n    \\n    def initialize_pinecone(self, index_name: str):\\n        \"\"\"\\n        Initialize Pinecone connection and verify if the index exists.\\n        \"\"\"\\n        # Check if the index already exists; if not, raise an error or handle accordingly\\n        if index_name not in [index.name for index in self.pinecone.list_indexes().indexes]:\\n            raise ValueError(f\"Index \\'{index_name}\\' does not exist. Please create it first.\")\\n\\n        # Connect to the specified index\\n        pinecone_index = self.pinecone.Index(index_name)\\n\\n        return pinecone_index\\n\\n    def perform_rag(self, pinecone_index, namespace, query):\\n        \"\"\"\\n        Perform Retrieval-Augmented Generation (RAG) by querying Pinecone and using Groq for response.\\n        \"\"\"\\n        # Get the embeddings for the query\\n        raw_query_embedding = self.get_huggingface_embeddings(query)\\n\\n        query_embedding = np.array(raw_query_embedding)\\n\\n        # Query Pinecone to retrieve top matches\\n        top_matches = pinecone_index.query(vector=query_embedding.tolist(), top_k=10, include_metadata=True, namespace=namespace)\\n\\n        # Extract contexts from the matches\\n        contexts = [item[\\'metadata\\'][\\'text\\'] for item in top_matches[\\'matches\\']]\\n\\n        # Augment the query with the context information\\n        augmented_query = \"<CONTEXT>\\\\n\" + \"\\\\n\\\\n-------\\\\n\\\\n\".join(contexts[:10]) + \"\\\\n-------\\\\n</CONTEXT>\\\\n\\\\n\\\\n\\\\nMY QUESTION:\\\\n\" + query\\n\\n        conversations=\"\\\\n\".join([f\"{msg[\\'role\\'].upper()}:{msg[\\'content\\']}\" for msg in self.conversation_history])\\n\\n        augmented_query += f\"Conversation History:\\\\n{conversations}\\\\n\\\\nMy Question:\\\\n{query}\"\\n        # Define the system prompt for Groq\\n        system_prompt = \\'\\'\\'\\n            You are an expert in reading the transcript of the given youtube video.\\n        Answer any question I have based on the transcripts i have provided. Mention the timestamps where the answer is given.\\n        Convert all the timestamps into minutes that are currently in seconds.\\n        \\'\\'\\'\\n\\n        # Make the call to Groq\\'s chat completions\\n        res = self.groq_client.chat.completions.create(\\n            model=\"llama-3.1-70b-versatile\", # Specify the Groq model\\n            messages=[\\n                {\"role\": \"system\", \"content\": system_prompt},\\n                {\"role\": \"user\", \"content\": augmented_query}\\n            ]\\n        )\\n\\n        bot_response= res.choices[0].message.content\\n\\n        #Update conversations\\n        self.conversation_history.append({\\'role\\':\\'user\\',\\'content\\':query})\\n        self.conversation_history.append({\\'role\\':\\'assistant\\',\\'content\\':bot_response})\\n\\n        return bot_response\\n    \\n    def reset_memory(self):\\n        \"\"\"\\n        Clear the conversation history.\\n        \"\"\"\\n        self.conversation_history = []\\n'},\n",
              " {'name': 'groq_utils/__init__.py',\n",
              "  'content': 'from .GroqClass import GroqClass'}]"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_huggingface_embeddings(text, model_name=\"sentence-transformers/all-mpnet-base-v2\"):\n",
        "    model = SentenceTransformer(model_name)\n",
        "    return model.encode(text)"
      ],
      "metadata": {
        "id": "7QGaKlp5ZWdJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"I like coding\"\n",
        "\n",
        "embeddings = get_huggingface_embeddings(text)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "CoNA4JTeZZAA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the PINECONE_API_KEY as an environment variable\n",
        "pinecone_api_key = userdata.get(\"PINECONE_API_KEY\")\n",
        "os.environ['PINECONE_API_KEY'] = pinecone_api_key\n",
        "\n",
        "# Initialize Pinecone\n",
        "pc = Pinecone(api_key=userdata.get(\"PINECONE_API_KEY\"),)\n",
        "\n",
        "# Connect to your Pinecone index\n",
        "pinecone_index = pc.Index(\"codebase-rag\")"
      ],
      "metadata": {
        "id": "6P2njYSMahib"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorstore = PineconeVectorStore(index_name=\"codebase-rag\", embedding=HuggingFaceEmbeddings())\n"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JUq7qMSPapGD",
        "outputId": "0f5c06da-b490-4108-c354-5ea01b11666c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-47-5982ffb8f713>:1: LangChainDeprecationWarning: Default values for HuggingFaceEmbeddings.model_name were deprecated in LangChain 0.2.16 and will be removed in 0.4.0. Explicitly pass a model_name to the HuggingFaceEmbeddings constructor instead.\n",
            "  vectorstore = PineconeVectorStore(index_name=\"codebase-rag\", embedding=HuggingFaceEmbeddings())\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert the codebase embeddings into Pinecone\n",
        "\n",
        "documents = []\n",
        "\n",
        "for file in files_content:\n",
        "    doc = Document(\n",
        "        page_content=f\"{file['name']}\\n\\n{file['content']}\",\n",
        "        metadata={\"source\": file['name']}\n",
        "    )\n",
        "\n",
        "    documents.append(doc)\n",
        "\n",
        "\n",
        "vectorstore = PineconeVectorStore.from_documents(\n",
        "    documents=documents,\n",
        "    embedding=HuggingFaceEmbeddings(),\n",
        "    index_name=\"codebase-rag\",\n",
        "    # namespace=\"https://github.com/CoderAgent/SecureAgent\"\n",
        "    namespace=github_repo\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T4cGcXTsasLM",
        "outputId": "d5793efd-31ce-4249-8381-eb34828e9f21"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-49-eb273c5678ab>:16: LangChainDeprecationWarning: Default values for HuggingFaceEmbeddings.model_name were deprecated in LangChain 0.2.16 and will be removed in 0.4.0. Explicitly pass a model_name to the HuggingFaceEmbeddings constructor instead.\n",
            "  embedding=HuggingFaceEmbeddings(),\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "documents"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "SRtA41eDaufj",
        "outputId": "6cdda10f-702c-42c9-8285-a8d284b4b0a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={'source': 'app.py', 'text': 'app.py\\n\\nfrom flask import Flask, render_template, request, jsonify\\nfrom groq_utils import GroqClass\\nfrom preprocessing import DocumentProcessor\\nfrom transcriber import YouTubeTranscriber\\nfrom langchain_community.embeddings import HuggingFaceEmbeddings\\nimport os\\nfrom dotenv import load_dotenv\\napp = Flask(__name__)\\n# Load environment variables from .env file\\nload_dotenv()\\n\\nindex_name = \"insightbot\"\\nnamespace = \"transcripts\"\\n\\n# Initialize embeddings and shared classes\\nembeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\\ngroq_client = GroqClass()  # Initialize GroqClass globally to be reused\\ndocument_processor = DocumentProcessor(index_name=index_name,namespace=namespace)  # Initialize DocumentProcessor globpally\\n# GLOBAL\\ndocument_dir_path = \"resources/documents\"\\ntranscript_dir_path = \"resources/transcripts\"\\n\\n@app.route(\\'/\\')\\ndef home():\\n    return render_template(\\'index.html\\')\\n\\n@app.route(\\'/chatbot\\')\\ndef chatbot():\\n    return render_template(\\'chatbot.html\\')\\n\\n@app.route(\\'/submit_media\\', methods=[\\'POST\\'])\\ndef submit_media():\\n    try:\\n        groq_client.reset_memory()\\n        document_processor.delete_files_in_directory(transcript_dir_path)\\n        document_processor.delete_files_in_directory(document_dir_path)\\n\\n        # Get YouTube links and documents from the request\\n        youtube_links = request.form.getlist(\\'youtube_links[]\\')  # List of YouTube links\\n        files = request.files.getlist(\\'documents[]\\')  # List of uploaded files\\n\\n        # Validate input\\n        if not youtube_links and not files:\\n            return jsonify({\"error\": \"At least one YouTube link or document is required\"}), 400\\n\\n        all_documents = []\\n\\n        # Process each YouTube link\\n        if youtube_links:\\n            print(\"=== Processing YouTube Links ===\")\\n            for youtube_link in youtube_links:\\n                if youtube_link:\\n                    print(f\"Processing YouTube Link: {youtube_link}\")\\n                    transcriber = YouTubeTranscriber(youtube_link, output_dir = transcript_dir_path)\\n                    transcriber.transcribe()\\n                    print(f\"Transcription complete for: {youtube_link}\")\\n            documents = document_processor.process_directory(transcript_dir_path)\\n            all_documents.extend(documents)\\n\\n        # Process uploaded documents\\n        if files:\\n            print(\"=== Processing Uploaded Documents ===\")\\n            for file in files:\\n                file_path = os.path.join(document_dir_path, file.filename)\\n                file.save(file_path)\\n                print(f\"Saved file: {file_path}\")\\n            uploaded_documents = document_processor.process_directory(document_dir_path)\\n            all_documents.extend(uploaded_documents)\\n\\n        if not all_documents:\\n            return jsonify({\"error\": \"No valid documents to process\"}), 400\\n\\n        # Prepare and chunk data\\n        document_data = document_processor.prepare_data(all_documents)\\n        documents_chunks = document_processor.chunk_data(document_data)\\n\\n        vectorstore_from_documents = document_processor.upsert_vectorstore_to_pinecone(documents_chunks, embeddings, index_name, namespace)\\n        print(\\'=== Upsert to Pinecone done ===\\', vectorstore_from_documents)\\n\\n        return jsonify({\"message\": \"Media processed successfully!\"})\\n\\n    except Exception as e:\\n        print(f\"Error: {str(e)}\")\\n        return jsonify({\"error\": \"Failed to process media\", \"details\": str(e)}), 500\\n\\n@app.route(\\'/submit_youtube_link\\', methods=[\\'POST\\'])\\n\\n@app.route(\\'/ask_question\\', methods=[\\'POST\\'])\\ndef ask_question():\\n    question = request.form.get(\\'question\\')\\n    # Get the response from the model\\n    pinecone_index = groq_client.initialize_pinecone(index_name)\\n    answer=groq_client.perform_rag(pinecone_index, namespace, question)\\n    return jsonify({\"question\": question, \"answer\": answer})\\n\\n# Simulating model response for now\\ndef get_model_response(question):\\n    # Replace with actual logic to handle YouTube video and query\\n    return f\"Answer to your question \\'{question}\\'\"\\n\\n\\n# if __name__ == \"__main__\":\\n#     app.run(debug=False)\\nif __name__==\"__main__\":\\n    app.run(host=os.getenv(\\'IP\\', \\'0.0.0.0\\'), \\n            port=int(os.getenv(\\'PORT\\', 4444)))'}, page_content='app.py\\n\\nfrom flask import Flask, render_template, request, jsonify\\nfrom groq_utils import GroqClass\\nfrom preprocessing import DocumentProcessor\\nfrom transcriber import YouTubeTranscriber\\nfrom langchain_community.embeddings import HuggingFaceEmbeddings\\nimport os\\nfrom dotenv import load_dotenv\\napp = Flask(__name__)\\n# Load environment variables from .env file\\nload_dotenv()\\n\\nindex_name = \"insightbot\"\\nnamespace = \"transcripts\"\\n\\n# Initialize embeddings and shared classes\\nembeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\\ngroq_client = GroqClass()  # Initialize GroqClass globally to be reused\\ndocument_processor = DocumentProcessor(index_name=index_name,namespace=namespace)  # Initialize DocumentProcessor globpally\\n# GLOBAL\\ndocument_dir_path = \"resources/documents\"\\ntranscript_dir_path = \"resources/transcripts\"\\n\\n@app.route(\\'/\\')\\ndef home():\\n    return render_template(\\'index.html\\')\\n\\n@app.route(\\'/chatbot\\')\\ndef chatbot():\\n    return render_template(\\'chatbot.html\\')\\n\\n@app.route(\\'/submit_media\\', methods=[\\'POST\\'])\\ndef submit_media():\\n    try:\\n        groq_client.reset_memory()\\n        document_processor.delete_files_in_directory(transcript_dir_path)\\n        document_processor.delete_files_in_directory(document_dir_path)\\n\\n        # Get YouTube links and documents from the request\\n        youtube_links = request.form.getlist(\\'youtube_links[]\\')  # List of YouTube links\\n        files = request.files.getlist(\\'documents[]\\')  # List of uploaded files\\n\\n        # Validate input\\n        if not youtube_links and not files:\\n            return jsonify({\"error\": \"At least one YouTube link or document is required\"}), 400\\n\\n        all_documents = []\\n\\n        # Process each YouTube link\\n        if youtube_links:\\n            print(\"=== Processing YouTube Links ===\")\\n            for youtube_link in youtube_links:\\n                if youtube_link:\\n                    print(f\"Processing YouTube Link: {youtube_link}\")\\n                    transcriber = YouTubeTranscriber(youtube_link, output_dir = transcript_dir_path)\\n                    transcriber.transcribe()\\n                    print(f\"Transcription complete for: {youtube_link}\")\\n            documents = document_processor.process_directory(transcript_dir_path)\\n            all_documents.extend(documents)\\n\\n        # Process uploaded documents\\n        if files:\\n            print(\"=== Processing Uploaded Documents ===\")\\n            for file in files:\\n                file_path = os.path.join(document_dir_path, file.filename)\\n                file.save(file_path)\\n                print(f\"Saved file: {file_path}\")\\n            uploaded_documents = document_processor.process_directory(document_dir_path)\\n            all_documents.extend(uploaded_documents)\\n\\n        if not all_documents:\\n            return jsonify({\"error\": \"No valid documents to process\"}), 400\\n\\n        # Prepare and chunk data\\n        document_data = document_processor.prepare_data(all_documents)\\n        documents_chunks = document_processor.chunk_data(document_data)\\n\\n        vectorstore_from_documents = document_processor.upsert_vectorstore_to_pinecone(documents_chunks, embeddings, index_name, namespace)\\n        print(\\'=== Upsert to Pinecone done ===\\', vectorstore_from_documents)\\n\\n        return jsonify({\"message\": \"Media processed successfully!\"})\\n\\n    except Exception as e:\\n        print(f\"Error: {str(e)}\")\\n        return jsonify({\"error\": \"Failed to process media\", \"details\": str(e)}), 500\\n\\n@app.route(\\'/submit_youtube_link\\', methods=[\\'POST\\'])\\n\\n@app.route(\\'/ask_question\\', methods=[\\'POST\\'])\\ndef ask_question():\\n    question = request.form.get(\\'question\\')\\n    # Get the response from the model\\n    pinecone_index = groq_client.initialize_pinecone(index_name)\\n    answer=groq_client.perform_rag(pinecone_index, namespace, question)\\n    return jsonify({\"question\": question, \"answer\": answer})\\n\\n# Simulating model response for now\\ndef get_model_response(question):\\n    # Replace with actual logic to handle YouTube video and query\\n    return f\"Answer to your question \\'{question}\\'\"\\n\\n\\n# if __name__ == \"__main__\":\\n#     app.run(debug=False)\\nif __name__==\"__main__\":\\n    app.run(host=os.getenv(\\'IP\\', \\'0.0.0.0\\'), \\n            port=int(os.getenv(\\'PORT\\', 4444)))'),\n",
              " Document(metadata={'source': 'static/script.js', 'text': \"static/script.js\\n\\n\\n// Function to handle question submission and display chat\\nfunction askQuestion() {\\n    const question = document.getElementById('question-input').value;\\n\\n    if (!question) {\\n        alert('Please ask a question');\\n        return;\\n    }\\n\\n    const chatBox = document.getElementById('chat-box');\\n\\n    // Display the question in chat\\n    const questionElement = document.createElement('div');\\n    questionElement.className = 'chat-message question';\\n    questionElement.textContent = question;\\n    chatBox.appendChild(questionElement);\\n\\n    // Clear the input\\n    document.getElementById('question-input').value = '';\\n\\n    // Get the model response\\n    fetch('/ask_question', {\\n        method: 'POST',\\n        headers: {\\n            'Content-Type': 'application/x-www-form-urlencoded'\\n        },\\n        body: `question=${question}`\\n    })\\n    .then(response => response.json())\\n    .then(data => {\\n        // Display the model's answer in chat\\n        const answerElement = document.createElement('div');\\n        answerElement.className = 'chat-message answer';\\n        answerElement.textContent = data.answer;\\n        chatBox.appendChild(answerElement);\\n\\n        // Scroll to the bottom of the chat\\n        chatBox.scrollTop = chatBox.scrollHeight;\\n    })\\n    .catch(error => console.error('Error:', error));\\n}\\n\\n// Function to clear chat \\nfunction clearChat() {\\n    // Clear chat messages\\n    document.getElementById('chat-box').innerHTML = '';\\n}\\n\\n// Store selected media\\nconst mediaItems = [];\\n\\n// Add YouTube Link\\nfunction addYoutubeLink() {\\n    const link = document.getElementById('youtube-link').value;\\n    if (link) {\\n        mediaItems.push({ type: 'YouTube', value: link });\\n        updateMediaList();\\n        document.getElementById('youtube-link').value = '';\\n    } else {\\n        alert('Please enter a valid YouTube link.');\\n    }\\n}\\n\\n// Add Uploaded Documents\\nfunction addDocuments() {\\n    const files = document.getElementById('upload-docs').files;\\n    for (let i = 0; i < files.length; i++) {\\n        // Push the entire file object, not just the name\\n        mediaItems.push({ type: 'Document', value: files[i] });\\n    }\\n    updateMediaList();\\n}\\n\\n// Update the Media List Display\\nfunction updateMediaList() {\\n    const list = document.getElementById('media-items');\\n    list.innerHTML = '';\\n    mediaItems.forEach(item => {\\n        const li = document.createElement('li');\\n        li.textContent = `${item.type}: ${\\n            item.type === 'Document' ? item.value.name : item.value\\n        }`;\\n        list.appendChild(li);\\n    });\\n}\\n\\n// Submit and process all media\\nfunction submitMedia() {\\n    const formData = new FormData();\\n\\n    // Add YouTube Links and Documents to FormData\\n    mediaItems.forEach(item => {\\n        if (item.type === 'YouTube') {\\n            formData.append('youtube_links[]', item.value);\\n        } else if (item.type === 'Document') {\\n            // Use the actual file object\\n            formData.append('documents[]', item.value);\\n        }\\n    });\\n\\n    // Debug: Log form data entries\\n    for (const [key, value] of formData.entries()) {\\n        console.log(`${key}:`, value);\\n    }\\n\\n    // Send FormData to the server\\n    return fetch('/submit_media', {\\n        method: 'POST',\\n        body: formData,\\n    });\\n}\\n\\n// Navigate to Chatbot Page\\nasync function goToChatbot() {\\n    if (mediaItems.length > 0) {\\n        try {\\n            // Call submitMedia and wait for the server response\\n            const response = await submitMedia();\\n            \\n            if (response.ok) {\\n                // Redirect to the chatbot page if submission is successful\\n                window.location.href = '/chatbot';\\n            } else {\\n                // Handle errors from the server\\n                const errorData = await response.json();\\n                alert(`Error: ${errorData.error || 'Failed to submit media.'}`);\\n            }\\n        } catch (error) {\\n            console.error('Error submitting media:', error);\\n            alert('An error occurred while submitting media. Please try again.');\\n        }\\n    } else {\\n        alert('Please add at least one media item.');\\n    }\\n}\\n\"}, page_content=\"static/script.js\\n\\n\\n// Function to handle question submission and display chat\\nfunction askQuestion() {\\n    const question = document.getElementById('question-input').value;\\n\\n    if (!question) {\\n        alert('Please ask a question');\\n        return;\\n    }\\n\\n    const chatBox = document.getElementById('chat-box');\\n\\n    // Display the question in chat\\n    const questionElement = document.createElement('div');\\n    questionElement.className = 'chat-message question';\\n    questionElement.textContent = question;\\n    chatBox.appendChild(questionElement);\\n\\n    // Clear the input\\n    document.getElementById('question-input').value = '';\\n\\n    // Get the model response\\n    fetch('/ask_question', {\\n        method: 'POST',\\n        headers: {\\n            'Content-Type': 'application/x-www-form-urlencoded'\\n        },\\n        body: `question=${question}`\\n    })\\n    .then(response => response.json())\\n    .then(data => {\\n        // Display the model's answer in chat\\n        const answerElement = document.createElement('div');\\n        answerElement.className = 'chat-message answer';\\n        answerElement.textContent = data.answer;\\n        chatBox.appendChild(answerElement);\\n\\n        // Scroll to the bottom of the chat\\n        chatBox.scrollTop = chatBox.scrollHeight;\\n    })\\n    .catch(error => console.error('Error:', error));\\n}\\n\\n// Function to clear chat \\nfunction clearChat() {\\n    // Clear chat messages\\n    document.getElementById('chat-box').innerHTML = '';\\n}\\n\\n// Store selected media\\nconst mediaItems = [];\\n\\n// Add YouTube Link\\nfunction addYoutubeLink() {\\n    const link = document.getElementById('youtube-link').value;\\n    if (link) {\\n        mediaItems.push({ type: 'YouTube', value: link });\\n        updateMediaList();\\n        document.getElementById('youtube-link').value = '';\\n    } else {\\n        alert('Please enter a valid YouTube link.');\\n    }\\n}\\n\\n// Add Uploaded Documents\\nfunction addDocuments() {\\n    const files = document.getElementById('upload-docs').files;\\n    for (let i = 0; i < files.length; i++) {\\n        // Push the entire file object, not just the name\\n        mediaItems.push({ type: 'Document', value: files[i] });\\n    }\\n    updateMediaList();\\n}\\n\\n// Update the Media List Display\\nfunction updateMediaList() {\\n    const list = document.getElementById('media-items');\\n    list.innerHTML = '';\\n    mediaItems.forEach(item => {\\n        const li = document.createElement('li');\\n        li.textContent = `${item.type}: ${\\n            item.type === 'Document' ? item.value.name : item.value\\n        }`;\\n        list.appendChild(li);\\n    });\\n}\\n\\n// Submit and process all media\\nfunction submitMedia() {\\n    const formData = new FormData();\\n\\n    // Add YouTube Links and Documents to FormData\\n    mediaItems.forEach(item => {\\n        if (item.type === 'YouTube') {\\n            formData.append('youtube_links[]', item.value);\\n        } else if (item.type === 'Document') {\\n            // Use the actual file object\\n            formData.append('documents[]', item.value);\\n        }\\n    });\\n\\n    // Debug: Log form data entries\\n    for (const [key, value] of formData.entries()) {\\n        console.log(`${key}:`, value);\\n    }\\n\\n    // Send FormData to the server\\n    return fetch('/submit_media', {\\n        method: 'POST',\\n        body: formData,\\n    });\\n}\\n\\n// Navigate to Chatbot Page\\nasync function goToChatbot() {\\n    if (mediaItems.length > 0) {\\n        try {\\n            // Call submitMedia and wait for the server response\\n            const response = await submitMedia();\\n            \\n            if (response.ok) {\\n                // Redirect to the chatbot page if submission is successful\\n                window.location.href = '/chatbot';\\n            } else {\\n                // Handle errors from the server\\n                const errorData = await response.json();\\n                alert(`Error: ${errorData.error || 'Failed to submit media.'}`);\\n            }\\n        } catch (error) {\\n            console.error('Error submitting media:', error);\\n            alert('An error occurred while submitting media. Please try again.');\\n        }\\n    } else {\\n        alert('Please add at least one media item.');\\n    }\\n}\\n\"),\n",
              " Document(metadata={'source': 'preprocessing/Prepare.py', 'text': 'preprocessing/Prepare.py\\n\\nimport os\\nimport time\\nfrom langchain_community.document_loaders import TextLoader\\nfrom langchain.schema import Document\\nfrom langchain_pinecone import PineconeVectorStore\\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\\nimport pinecone\\nfrom dotenv import load_dotenv\\nfrom pinecone import Pinecone\\nfrom pypdf import PdfReader\\nfrom docx import Document as DocxDocument\\nfrom langchain.schema import Document as LangchainDocument\\n\\n# Load environment variables\\nload_dotenv()\\n# os.environ[\\'PINECONE_API_KEY\\'] = os.environ.get(\\'PINECONE_API_KEY\\')\\n\\nclass DocumentProcessor:\\n    def __init__(self, index_name: str, namespace: str, chunk_size: int = 1000, chunk_overlap: int = 30):\\n        self.index_name = index_name\\n        self.namespace = namespace\\n        self.chunk_size = chunk_size\\n        self.chunk_overlap = chunk_overlap\\n\\n    def delete_files_in_directory(self, directory_path: str):\\n        \"\"\"\\n        Delete all files in a given directory.\\n        \"\"\"\\n        files = os.listdir(directory_path)\\n        print(\\'Files in dir:\\', files)\\n        for file in files:\\n            file_path = os.path.join(directory_path, file)\\n            if os.path.isfile(file_path):\\n                os.remove(file_path)\\n        print(\"All files deleted successfully.\")\\n\\n    def process_directory(self, directory_path: str):\\n        data = []\\n        \\n        for root, _, files in os.walk(directory_path):\\n            for file in files:\\n                file_path = os.path.join(root, file)\\n                print(f\"Processing file: {file_path}\")\\n                \\n                # Skip empty files\\n                if os.path.getsize(file_path) == 0:\\n                    print(f\"Warning: The file {file_path} is empty and will be skipped.\")\\n                    continue\\n                \\n                try:\\n                    # Process .txt files\\n                    if file.endswith(\".txt\"):\\n                        with open(file_path, \"r\", encoding=\"utf-8\") as f:\\n                            file_data = f.read()\\n                    \\n                    # Process .pdf files\\n                    elif file.endswith(\".pdf\"):\\n                        reader = PdfReader(file_path)\\n                        file_data = \"\"\\n                        for page in reader.pages:\\n                            file_data += page.extract_text()\\n                    \\n                    # Process .docx files\\n                    elif file.endswith(\".docx\"):\\n                        document = DocxDocument(file_path)\\n                        file_data = \"\\\\n\".join([para.text for para in document.paragraphs])\\n                    \\n                    else:\\n                        print(f\"Unsupported file format: {file}\")\\n                        continue\\n                    \\n                    # Ensure valid content was extracted\\n                    if not file_data.strip():\\n                        print(f\"Warning: No content extracted from {file_path}. Skipping.\")\\n                        continue\\n                    \\n                    # Append the processed data\\n                    data.append({\"File\": file_path, \"Data\": file_data})\\n                \\n                except Exception as e:\\n                    print(f\"Error processing file {file_path}: {e}\")\\n        return data\\n\\n    def prepare_data(self, documents):\\n        # Prepare the text for embedding\\n        document_data = []\\n        for document in documents:\\n            # Ensure \\'Data\\' exists and is non-empty\\n            if \\'Data\\' not in document or not document[\\'Data\\']:\\n                print(f\"Skipping document due to missing or empty \\'Data\\': {document}\")\\n                continue\\n            \\n            # Ensure the first element in \\'Data\\' is a Document object\\n            if not isinstance(document[\\'Data\\'][0], str):\\n                print(f\"Skipping document due to invalid data type in \\'Data\\': {document}\")\\n                continue\\n\\n            # Extract metadata and content\\n            document_source = document[\\'File\\']\\n            document_content = document[\\'Data\\']\\n\\n            file_name = document_source.split(\"/\")[-1]\\n            folder_names = document_source.split(\"/\")[2:-1] if \"/\" in document_source else []\\n\\n            doc = LangchainDocument(\\n                page_content=f\"<Source>\\\\n{document_source}\\\\n</Source>\\\\n\\\\n<Content>\\\\n{document_content}\\\\n</Content>\",\\n                metadata={\\n                    \"file_name\": file_name,\\n                    \"parent_folder\": folder_names[-1] if folder_names else \"\",\\n                    \"folder_names\": folder_names\\n                }\\n            )\\n            document_data.append(doc)\\n\\n        return document_data\\n\\n    def chunk_data(self, docs):\\n        \"\"\"\\n        Split documents into chunks based on the provided chunk size and overlap.\\n        \"\"\"\\n        textsplitter = RecursiveCharacterTextSplitter(chunk_size=self.chunk_size, chunk_overlap=self.chunk_overlap)\\n        docs = textsplitter.split_documents(docs)\\n        return docs\\n\\n    def upsert_vectorstore_to_pinecone(self, document_data, embeddings, index_name, namespace):\\n        # # Initialize Pinecone connection with the new API structure\\n        pc = pinecone.Pinecone(api_key=os.environ.get(\\'PINECONE_API_KEY\\'))\\n\\n        # Check if the namespace exists in the index\\n        index = pc.Index(self.index_name)\\n\\n        # Check if the namespace exists by listing the namespaces (or by trying to query)\\n        namespaces = index.describe_index_stats().get(\\'namespaces\\', [])\\n        max_retries = 5\\n        wait_time = 2000\\n        if self.namespace in namespaces:\\n            print(f\"Namespace \\'{self.namespace}\\' found. Deleting vector data...\")\\n            index.delete(namespace=self.namespace, delete_all=True)  # Initiates deletion\\n\\n            # Polling to ensure deletion completes\\n            for attempt in range(max_retries):\\n                namespaces = index.describe_index_stats().get(\\'namespaces\\', [])\\n                if self.namespace not in namespaces:\\n                    print(f\"Namespace \\'{self.namespace}\\' deletion confirmed.\")\\n                    break\\n                time.sleep(wait_time)  # Wait before re-checking\\n            else:\\n                raise RuntimeError(f\"Failed to delete namespace \\'{self.namespace}\\' after {max_retries} retries.\")\\n\\n        else:\\n            print(f\"Namespace \\'{self.namespace}\\' does not exist. Proceeding with upsert.\")\\n\\n        # Create or replace the vector store\\n        vectorstore_from_documents = PineconeVectorStore.from_documents(\\n            document_data,\\n            embeddings,\\n            index_name=self.index_name,\\n            namespace=self.namespace\\n        )\\n        print(f\"Vector store type: {type(vectorstore_from_documents)}\")\\n\\n        # Optionally, return the vector store if needed\\n        return vectorstore_from_documents\\n\\n    def initialize_pinecone(self):\\n        \"\"\"\\n        Initialize Pinecone with the provided index name.\\n        \"\"\"\\n        # Initialize Pinecone instance\\n        pc = Pinecone(api_key=os.environ.get(\\'PINECONE_API_KEY\\'))\\n\\n        # Check if the index exists\\n        if self.index_name not in [index.name for index in pc.list_indexes().indexes]:\\n            raise ValueError(f\"Index \\'{self.index_name}\\' does not exist. Please create it first.\")\\n\\n        # Connect to the specified index\\n        pinecone_index = pc.Index(self.index_name)\\n        return pinecone_index\\n'}, page_content='preprocessing/Prepare.py\\n\\nimport os\\nimport time\\nfrom langchain_community.document_loaders import TextLoader\\nfrom langchain.schema import Document\\nfrom langchain_pinecone import PineconeVectorStore\\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\\nimport pinecone\\nfrom dotenv import load_dotenv\\nfrom pinecone import Pinecone\\nfrom pypdf import PdfReader\\nfrom docx import Document as DocxDocument\\nfrom langchain.schema import Document as LangchainDocument\\n\\n# Load environment variables\\nload_dotenv()\\n# os.environ[\\'PINECONE_API_KEY\\'] = os.environ.get(\\'PINECONE_API_KEY\\')\\n\\nclass DocumentProcessor:\\n    def __init__(self, index_name: str, namespace: str, chunk_size: int = 1000, chunk_overlap: int = 30):\\n        self.index_name = index_name\\n        self.namespace = namespace\\n        self.chunk_size = chunk_size\\n        self.chunk_overlap = chunk_overlap\\n\\n    def delete_files_in_directory(self, directory_path: str):\\n        \"\"\"\\n        Delete all files in a given directory.\\n        \"\"\"\\n        files = os.listdir(directory_path)\\n        print(\\'Files in dir:\\', files)\\n        for file in files:\\n            file_path = os.path.join(directory_path, file)\\n            if os.path.isfile(file_path):\\n                os.remove(file_path)\\n        print(\"All files deleted successfully.\")\\n\\n    def process_directory(self, directory_path: str):\\n        data = []\\n        \\n        for root, _, files in os.walk(directory_path):\\n            for file in files:\\n                file_path = os.path.join(root, file)\\n                print(f\"Processing file: {file_path}\")\\n                \\n                # Skip empty files\\n                if os.path.getsize(file_path) == 0:\\n                    print(f\"Warning: The file {file_path} is empty and will be skipped.\")\\n                    continue\\n                \\n                try:\\n                    # Process .txt files\\n                    if file.endswith(\".txt\"):\\n                        with open(file_path, \"r\", encoding=\"utf-8\") as f:\\n                            file_data = f.read()\\n                    \\n                    # Process .pdf files\\n                    elif file.endswith(\".pdf\"):\\n                        reader = PdfReader(file_path)\\n                        file_data = \"\"\\n                        for page in reader.pages:\\n                            file_data += page.extract_text()\\n                    \\n                    # Process .docx files\\n                    elif file.endswith(\".docx\"):\\n                        document = DocxDocument(file_path)\\n                        file_data = \"\\\\n\".join([para.text for para in document.paragraphs])\\n                    \\n                    else:\\n                        print(f\"Unsupported file format: {file}\")\\n                        continue\\n                    \\n                    # Ensure valid content was extracted\\n                    if not file_data.strip():\\n                        print(f\"Warning: No content extracted from {file_path}. Skipping.\")\\n                        continue\\n                    \\n                    # Append the processed data\\n                    data.append({\"File\": file_path, \"Data\": file_data})\\n                \\n                except Exception as e:\\n                    print(f\"Error processing file {file_path}: {e}\")\\n        return data\\n\\n    def prepare_data(self, documents):\\n        # Prepare the text for embedding\\n        document_data = []\\n        for document in documents:\\n            # Ensure \\'Data\\' exists and is non-empty\\n            if \\'Data\\' not in document or not document[\\'Data\\']:\\n                print(f\"Skipping document due to missing or empty \\'Data\\': {document}\")\\n                continue\\n            \\n            # Ensure the first element in \\'Data\\' is a Document object\\n            if not isinstance(document[\\'Data\\'][0], str):\\n                print(f\"Skipping document due to invalid data type in \\'Data\\': {document}\")\\n                continue\\n\\n            # Extract metadata and content\\n            document_source = document[\\'File\\']\\n            document_content = document[\\'Data\\']\\n\\n            file_name = document_source.split(\"/\")[-1]\\n            folder_names = document_source.split(\"/\")[2:-1] if \"/\" in document_source else []\\n\\n            doc = LangchainDocument(\\n                page_content=f\"<Source>\\\\n{document_source}\\\\n</Source>\\\\n\\\\n<Content>\\\\n{document_content}\\\\n</Content>\",\\n                metadata={\\n                    \"file_name\": file_name,\\n                    \"parent_folder\": folder_names[-1] if folder_names else \"\",\\n                    \"folder_names\": folder_names\\n                }\\n            )\\n            document_data.append(doc)\\n\\n        return document_data\\n\\n    def chunk_data(self, docs):\\n        \"\"\"\\n        Split documents into chunks based on the provided chunk size and overlap.\\n        \"\"\"\\n        textsplitter = RecursiveCharacterTextSplitter(chunk_size=self.chunk_size, chunk_overlap=self.chunk_overlap)\\n        docs = textsplitter.split_documents(docs)\\n        return docs\\n\\n    def upsert_vectorstore_to_pinecone(self, document_data, embeddings, index_name, namespace):\\n        # # Initialize Pinecone connection with the new API structure\\n        pc = pinecone.Pinecone(api_key=os.environ.get(\\'PINECONE_API_KEY\\'))\\n\\n        # Check if the namespace exists in the index\\n        index = pc.Index(self.index_name)\\n\\n        # Check if the namespace exists by listing the namespaces (or by trying to query)\\n        namespaces = index.describe_index_stats().get(\\'namespaces\\', [])\\n        max_retries = 5\\n        wait_time = 2000\\n        if self.namespace in namespaces:\\n            print(f\"Namespace \\'{self.namespace}\\' found. Deleting vector data...\")\\n            index.delete(namespace=self.namespace, delete_all=True)  # Initiates deletion\\n\\n            # Polling to ensure deletion completes\\n            for attempt in range(max_retries):\\n                namespaces = index.describe_index_stats().get(\\'namespaces\\', [])\\n                if self.namespace not in namespaces:\\n                    print(f\"Namespace \\'{self.namespace}\\' deletion confirmed.\")\\n                    break\\n                time.sleep(wait_time)  # Wait before re-checking\\n            else:\\n                raise RuntimeError(f\"Failed to delete namespace \\'{self.namespace}\\' after {max_retries} retries.\")\\n\\n        else:\\n            print(f\"Namespace \\'{self.namespace}\\' does not exist. Proceeding with upsert.\")\\n\\n        # Create or replace the vector store\\n        vectorstore_from_documents = PineconeVectorStore.from_documents(\\n            document_data,\\n            embeddings,\\n            index_name=self.index_name,\\n            namespace=self.namespace\\n        )\\n        print(f\"Vector store type: {type(vectorstore_from_documents)}\")\\n\\n        # Optionally, return the vector store if needed\\n        return vectorstore_from_documents\\n\\n    def initialize_pinecone(self):\\n        \"\"\"\\n        Initialize Pinecone with the provided index name.\\n        \"\"\"\\n        # Initialize Pinecone instance\\n        pc = Pinecone(api_key=os.environ.get(\\'PINECONE_API_KEY\\'))\\n\\n        # Check if the index exists\\n        if self.index_name not in [index.name for index in pc.list_indexes().indexes]:\\n            raise ValueError(f\"Index \\'{self.index_name}\\' does not exist. Please create it first.\")\\n\\n        # Connect to the specified index\\n        pinecone_index = pc.Index(self.index_name)\\n        return pinecone_index\\n'),\n",
              " Document(metadata={'source': 'preprocessing/__init__.py', 'text': 'preprocessing/__init__.py\\n\\nfrom .Prepare import DocumentProcessor'}, page_content='preprocessing/__init__.py\\n\\nfrom .Prepare import DocumentProcessor'),\n",
              " Document(metadata={'source': 'transcriber/__init__.py', 'text': 'transcriber/__init__.py\\n\\nfrom .youtube_transcriber import YouTubeTranscriber\\n'}, page_content='transcriber/__init__.py\\n\\nfrom .youtube_transcriber import YouTubeTranscriber\\n'),\n",
              " Document(metadata={'source': 'transcriber/youtube_transcriber.py', 'text': 'transcriber/youtube_transcriber.py\\n\\nimport os\\nfrom pytube import Playlist\\nfrom youtube_transcript_api import YouTubeTranscriptApi\\nimport ssl\\nclass YouTubeTranscriber:\\n    def __init__(self, url, transcript_language=\\'en\\', output_dir=\\'resources/transcripts\\'):\\n        \"\"\"\\n        Initializes the YouTubeTranscriber with a URL (playlist or single video), transcript language, and output directory.\\n\\n        :param url: URL of the YouTube playlist or single video\\n        :param transcript_language: Preferred language for transcripts (default is \\'en\\' for English)\\n        :param output_dir: Directory where transcripts will be saved\\n        \"\"\"\\n        self.url = url\\n        self.transcript_language = transcript_language\\n        self.output_dir = output_dir\\n\\n        # Check if the URL is for a playlist or a single video\\n        if \\'playlist?list=\\' in url:\\n            self.is_playlist = True\\n            self.playlist = []\\n            # Disable SSL verification\\n            ssl._create_default_https_context = ssl._create_unverified_context\\n            playlist_urls = Playlist(url)\\n            for url in playlist_urls:\\n                self.playlist.append(url)\\n        elif \\'watch?v=\\' in url or \\'youtu.be/\\' in url:\\n            self.is_playlist = False\\n            if \\'watch?v=\\' in url:\\n                # Extract video ID from \"https://www.youtube.com/watch?v=\"\\n                self.video_id = url.split(\\'watch?v=\\')[1].split(\\'&\\')[0]\\n            elif \\'youtu.be/\\' in url:\\n                # Extract video ID from \"https://youtu.be/\"\\n                self.video_id = url.split(\\'youtu.be/\\')[1].split(\\'?\\')[0]\\n        else:\\n            raise ValueError(\"Invalid URL. Provide a valid YouTube playlist or video URL.\")\\n\\n        # Ensure output directory exists\\n        os.makedirs(self.output_dir, exist_ok=True)\\n        if self.is_playlist:\\n            print(f\"Found {len(self.playlist)} videos in the playlist.\")\\n\\n    def fetch_transcript(self, video_id):\\n        \"\"\"\\n        Fetches the transcript for a single video in the specified language.\\n\\n        :param video_id: YouTube video ID\\n        :return: Transcript as a list of dictionaries with \\'start\\' and \\'text\\' keys\\n        \"\"\"\\n        try:\\n            # Fetch transcript with the specified language\\n            transcript = YouTubeTranscriptApi.get_transcript(video_id, languages=[self.transcript_language])\\n            return transcript\\n        except Exception as e:\\n            print(f\"Could not retrieve transcript for video ID {video_id}: {e}\")\\n            return None\\n\\n    def save_transcript_to_file(self, video_id, transcript):\\n        \"\"\"\\n        Saves the transcript of a video to a text file.\\n\\n        :param video_id: YouTube video ID\\n        :param transcript: Transcript data to save\\n        \"\"\"\\n        file_path = os.path.join(self.output_dir, f\"{video_id}_transcript.txt\")\\n        with open(file_path, \"w\", encoding=\"utf-8\") as file:\\n            for line in transcript:\\n                file.write(f\"{line[\\'start\\']}: {line[\\'text\\']}\\\\n\")\\n        print(f\"Transcript saved for video ID: {video_id}\")\\n\\n    def transcribe_playlist(self):\\n        \"\"\"\\n        Processes each video in the playlist to fetch and save transcripts.\\n        \"\"\"\\n        for video_url in self.playlist:\\n            # Extract video ID from URL\\n            video_id = video_url.split(\\'=\\')[-1]\\n            if \\'watch?v=\\' in video_url:\\n                # Extract video ID from \"https://www.youtube.com/watch?v=\"\\n                video_id = video_url.split(\\'watch?v=\\')[1].split(\\'&\\')[0]\\n            elif \\'youtu.be/\\' in video_url:\\n                # Extract video ID from \"https://youtu.be/\"\\n                video_id = video_url.split(\\'youtu.be/\\')[1].split(\\'?\\')[0]\\n            else:\\n                raise ValueError(\"Invalid URL. Provide a valid YouTube playlist or video URL.\")\\n            # Fetch and save the transcript\\n            transcript = self.fetch_transcript(video_id)\\n            if transcript:\\n                self.save_transcript_to_file(video_id, transcript)\\n\\n    def transcribe_single_video(self):\\n        \"\"\"\\n        Fetches and saves the transcript for a single YouTube video.\\n        \"\"\"\\n        # Fetch and save the transcript\\n        transcript = self.fetch_transcript(self.video_id)\\n        if transcript:\\n            self.save_transcript_to_file(self.video_id, transcript)\\n\\n    def transcribe(self):\\n        \"\"\"\\n        Determines if the URL is a playlist or single video and processes accordingly.\\n        \"\"\"\\n        if self.is_playlist:\\n            self.transcribe_playlist()\\n        else:\\n            self.transcribe_single_video()'}, page_content='transcriber/youtube_transcriber.py\\n\\nimport os\\nfrom pytube import Playlist\\nfrom youtube_transcript_api import YouTubeTranscriptApi\\nimport ssl\\nclass YouTubeTranscriber:\\n    def __init__(self, url, transcript_language=\\'en\\', output_dir=\\'resources/transcripts\\'):\\n        \"\"\"\\n        Initializes the YouTubeTranscriber with a URL (playlist or single video), transcript language, and output directory.\\n\\n        :param url: URL of the YouTube playlist or single video\\n        :param transcript_language: Preferred language for transcripts (default is \\'en\\' for English)\\n        :param output_dir: Directory where transcripts will be saved\\n        \"\"\"\\n        self.url = url\\n        self.transcript_language = transcript_language\\n        self.output_dir = output_dir\\n\\n        # Check if the URL is for a playlist or a single video\\n        if \\'playlist?list=\\' in url:\\n            self.is_playlist = True\\n            self.playlist = []\\n            # Disable SSL verification\\n            ssl._create_default_https_context = ssl._create_unverified_context\\n            playlist_urls = Playlist(url)\\n            for url in playlist_urls:\\n                self.playlist.append(url)\\n        elif \\'watch?v=\\' in url or \\'youtu.be/\\' in url:\\n            self.is_playlist = False\\n            if \\'watch?v=\\' in url:\\n                # Extract video ID from \"https://www.youtube.com/watch?v=\"\\n                self.video_id = url.split(\\'watch?v=\\')[1].split(\\'&\\')[0]\\n            elif \\'youtu.be/\\' in url:\\n                # Extract video ID from \"https://youtu.be/\"\\n                self.video_id = url.split(\\'youtu.be/\\')[1].split(\\'?\\')[0]\\n        else:\\n            raise ValueError(\"Invalid URL. Provide a valid YouTube playlist or video URL.\")\\n\\n        # Ensure output directory exists\\n        os.makedirs(self.output_dir, exist_ok=True)\\n        if self.is_playlist:\\n            print(f\"Found {len(self.playlist)} videos in the playlist.\")\\n\\n    def fetch_transcript(self, video_id):\\n        \"\"\"\\n        Fetches the transcript for a single video in the specified language.\\n\\n        :param video_id: YouTube video ID\\n        :return: Transcript as a list of dictionaries with \\'start\\' and \\'text\\' keys\\n        \"\"\"\\n        try:\\n            # Fetch transcript with the specified language\\n            transcript = YouTubeTranscriptApi.get_transcript(video_id, languages=[self.transcript_language])\\n            return transcript\\n        except Exception as e:\\n            print(f\"Could not retrieve transcript for video ID {video_id}: {e}\")\\n            return None\\n\\n    def save_transcript_to_file(self, video_id, transcript):\\n        \"\"\"\\n        Saves the transcript of a video to a text file.\\n\\n        :param video_id: YouTube video ID\\n        :param transcript: Transcript data to save\\n        \"\"\"\\n        file_path = os.path.join(self.output_dir, f\"{video_id}_transcript.txt\")\\n        with open(file_path, \"w\", encoding=\"utf-8\") as file:\\n            for line in transcript:\\n                file.write(f\"{line[\\'start\\']}: {line[\\'text\\']}\\\\n\")\\n        print(f\"Transcript saved for video ID: {video_id}\")\\n\\n    def transcribe_playlist(self):\\n        \"\"\"\\n        Processes each video in the playlist to fetch and save transcripts.\\n        \"\"\"\\n        for video_url in self.playlist:\\n            # Extract video ID from URL\\n            video_id = video_url.split(\\'=\\')[-1]\\n            if \\'watch?v=\\' in video_url:\\n                # Extract video ID from \"https://www.youtube.com/watch?v=\"\\n                video_id = video_url.split(\\'watch?v=\\')[1].split(\\'&\\')[0]\\n            elif \\'youtu.be/\\' in video_url:\\n                # Extract video ID from \"https://youtu.be/\"\\n                video_id = video_url.split(\\'youtu.be/\\')[1].split(\\'?\\')[0]\\n            else:\\n                raise ValueError(\"Invalid URL. Provide a valid YouTube playlist or video URL.\")\\n            # Fetch and save the transcript\\n            transcript = self.fetch_transcript(video_id)\\n            if transcript:\\n                self.save_transcript_to_file(video_id, transcript)\\n\\n    def transcribe_single_video(self):\\n        \"\"\"\\n        Fetches and saves the transcript for a single YouTube video.\\n        \"\"\"\\n        # Fetch and save the transcript\\n        transcript = self.fetch_transcript(self.video_id)\\n        if transcript:\\n            self.save_transcript_to_file(self.video_id, transcript)\\n\\n    def transcribe(self):\\n        \"\"\"\\n        Determines if the URL is a playlist or single video and processes accordingly.\\n        \"\"\"\\n        if self.is_playlist:\\n            self.transcribe_playlist()\\n        else:\\n            self.transcribe_single_video()'),\n",
              " Document(metadata={'source': 'groq_utils/GroqClass.py', 'text': 'groq_utils/GroqClass.py\\n\\nfrom sentence_transformers import SentenceTransformer\\nfrom groq import Groq\\nimport numpy as np\\nfrom pinecone import Pinecone\\nfrom langchain_community.embeddings import HuggingFaceEmbeddings\\nfrom dotenv import load_dotenv\\nimport os\\n\\nclass GroqClass:\\n    def __init__(self, model_name=\"sentence-transformers/all-MiniLM-L6-v2\"):\\n        \"\"\"\\n        Initialize the class with necessary keys and model for embeddings.\\n        \"\"\"\\n        # Load environment variables from .env file\\n        load_dotenv()\\n\\n        # Set environment variables from .env\\n\\n        # self.pinecone_api_key = os.environ.get(\\'PINECONE_API_KEY\\')\\n        # self.groq_api_key = os.environ.get(\\'GROQ_API_KEY\\')\\n        self.model_name = model_name\\n\\n        # print(f\"GROQ_API_KEY: {self.groq_api_key}\")\\n        # print(f\"PINECONE_API_KEY: {self.pinecone_api_key}\")\\n        # Initialize the Groq client with the provided API key\\n        self.groq_client = Groq(api_key=os.environ.get(\\'GROQ_API_KEY\\'))\\n\\n        # Initialize embeddings\\n        self.embeddings = HuggingFaceEmbeddings(model_name=self.model_name)\\n\\n        # Initialize Pinecone client\\n        self.pinecone = Pinecone(api_key=os.environ.get(\\'PINECONE_API_KEY\\'))\\n\\n        self.conversation_history=[]\\n\\n    def get_huggingface_embeddings(self, text):\\n        \"\"\"\\n        Get the Hugging Face embeddings for a given text.\\n        \"\"\"\\n        model = SentenceTransformer(self.model_name)\\n        return model.encode(text)\\n    \\n    def initialize_pinecone(self, index_name: str):\\n        \"\"\"\\n        Initialize Pinecone connection and verify if the index exists.\\n        \"\"\"\\n        # Check if the index already exists; if not, raise an error or handle accordingly\\n        if index_name not in [index.name for index in self.pinecone.list_indexes().indexes]:\\n            raise ValueError(f\"Index \\'{index_name}\\' does not exist. Please create it first.\")\\n\\n        # Connect to the specified index\\n        pinecone_index = self.pinecone.Index(index_name)\\n\\n        return pinecone_index\\n\\n    def perform_rag(self, pinecone_index, namespace, query):\\n        \"\"\"\\n        Perform Retrieval-Augmented Generation (RAG) by querying Pinecone and using Groq for response.\\n        \"\"\"\\n        # Get the embeddings for the query\\n        raw_query_embedding = self.get_huggingface_embeddings(query)\\n\\n        query_embedding = np.array(raw_query_embedding)\\n\\n        # Query Pinecone to retrieve top matches\\n        top_matches = pinecone_index.query(vector=query_embedding.tolist(), top_k=10, include_metadata=True, namespace=namespace)\\n\\n        # Extract contexts from the matches\\n        contexts = [item[\\'metadata\\'][\\'text\\'] for item in top_matches[\\'matches\\']]\\n\\n        # Augment the query with the context information\\n        augmented_query = \"<CONTEXT>\\\\n\" + \"\\\\n\\\\n-------\\\\n\\\\n\".join(contexts[:10]) + \"\\\\n-------\\\\n</CONTEXT>\\\\n\\\\n\\\\n\\\\nMY QUESTION:\\\\n\" + query\\n\\n        conversations=\"\\\\n\".join([f\"{msg[\\'role\\'].upper()}:{msg[\\'content\\']}\" for msg in self.conversation_history])\\n\\n        augmented_query += f\"Conversation History:\\\\n{conversations}\\\\n\\\\nMy Question:\\\\n{query}\"\\n        # Define the system prompt for Groq\\n        system_prompt = \\'\\'\\'\\n            You are an expert in reading the transcript of the given youtube video.\\n        Answer any question I have based on the transcripts i have provided. Mention the timestamps where the answer is given.\\n        Convert all the timestamps into minutes that are currently in seconds.\\n        \\'\\'\\'\\n\\n        # Make the call to Groq\\'s chat completions\\n        res = self.groq_client.chat.completions.create(\\n            model=\"llama-3.1-70b-versatile\", # Specify the Groq model\\n            messages=[\\n                {\"role\": \"system\", \"content\": system_prompt},\\n                {\"role\": \"user\", \"content\": augmented_query}\\n            ]\\n        )\\n\\n        bot_response= res.choices[0].message.content\\n\\n        #Update conversations\\n        self.conversation_history.append({\\'role\\':\\'user\\',\\'content\\':query})\\n        self.conversation_history.append({\\'role\\':\\'assistant\\',\\'content\\':bot_response})\\n\\n        return bot_response\\n    \\n    def reset_memory(self):\\n        \"\"\"\\n        Clear the conversation history.\\n        \"\"\"\\n        self.conversation_history = []\\n'}, page_content='groq_utils/GroqClass.py\\n\\nfrom sentence_transformers import SentenceTransformer\\nfrom groq import Groq\\nimport numpy as np\\nfrom pinecone import Pinecone\\nfrom langchain_community.embeddings import HuggingFaceEmbeddings\\nfrom dotenv import load_dotenv\\nimport os\\n\\nclass GroqClass:\\n    def __init__(self, model_name=\"sentence-transformers/all-MiniLM-L6-v2\"):\\n        \"\"\"\\n        Initialize the class with necessary keys and model for embeddings.\\n        \"\"\"\\n        # Load environment variables from .env file\\n        load_dotenv()\\n\\n        # Set environment variables from .env\\n\\n        # self.pinecone_api_key = os.environ.get(\\'PINECONE_API_KEY\\')\\n        # self.groq_api_key = os.environ.get(\\'GROQ_API_KEY\\')\\n        self.model_name = model_name\\n\\n        # print(f\"GROQ_API_KEY: {self.groq_api_key}\")\\n        # print(f\"PINECONE_API_KEY: {self.pinecone_api_key}\")\\n        # Initialize the Groq client with the provided API key\\n        self.groq_client = Groq(api_key=os.environ.get(\\'GROQ_API_KEY\\'))\\n\\n        # Initialize embeddings\\n        self.embeddings = HuggingFaceEmbeddings(model_name=self.model_name)\\n\\n        # Initialize Pinecone client\\n        self.pinecone = Pinecone(api_key=os.environ.get(\\'PINECONE_API_KEY\\'))\\n\\n        self.conversation_history=[]\\n\\n    def get_huggingface_embeddings(self, text):\\n        \"\"\"\\n        Get the Hugging Face embeddings for a given text.\\n        \"\"\"\\n        model = SentenceTransformer(self.model_name)\\n        return model.encode(text)\\n    \\n    def initialize_pinecone(self, index_name: str):\\n        \"\"\"\\n        Initialize Pinecone connection and verify if the index exists.\\n        \"\"\"\\n        # Check if the index already exists; if not, raise an error or handle accordingly\\n        if index_name not in [index.name for index in self.pinecone.list_indexes().indexes]:\\n            raise ValueError(f\"Index \\'{index_name}\\' does not exist. Please create it first.\")\\n\\n        # Connect to the specified index\\n        pinecone_index = self.pinecone.Index(index_name)\\n\\n        return pinecone_index\\n\\n    def perform_rag(self, pinecone_index, namespace, query):\\n        \"\"\"\\n        Perform Retrieval-Augmented Generation (RAG) by querying Pinecone and using Groq for response.\\n        \"\"\"\\n        # Get the embeddings for the query\\n        raw_query_embedding = self.get_huggingface_embeddings(query)\\n\\n        query_embedding = np.array(raw_query_embedding)\\n\\n        # Query Pinecone to retrieve top matches\\n        top_matches = pinecone_index.query(vector=query_embedding.tolist(), top_k=10, include_metadata=True, namespace=namespace)\\n\\n        # Extract contexts from the matches\\n        contexts = [item[\\'metadata\\'][\\'text\\'] for item in top_matches[\\'matches\\']]\\n\\n        # Augment the query with the context information\\n        augmented_query = \"<CONTEXT>\\\\n\" + \"\\\\n\\\\n-------\\\\n\\\\n\".join(contexts[:10]) + \"\\\\n-------\\\\n</CONTEXT>\\\\n\\\\n\\\\n\\\\nMY QUESTION:\\\\n\" + query\\n\\n        conversations=\"\\\\n\".join([f\"{msg[\\'role\\'].upper()}:{msg[\\'content\\']}\" for msg in self.conversation_history])\\n\\n        augmented_query += f\"Conversation History:\\\\n{conversations}\\\\n\\\\nMy Question:\\\\n{query}\"\\n        # Define the system prompt for Groq\\n        system_prompt = \\'\\'\\'\\n            You are an expert in reading the transcript of the given youtube video.\\n        Answer any question I have based on the transcripts i have provided. Mention the timestamps where the answer is given.\\n        Convert all the timestamps into minutes that are currently in seconds.\\n        \\'\\'\\'\\n\\n        # Make the call to Groq\\'s chat completions\\n        res = self.groq_client.chat.completions.create(\\n            model=\"llama-3.1-70b-versatile\", # Specify the Groq model\\n            messages=[\\n                {\"role\": \"system\", \"content\": system_prompt},\\n                {\"role\": \"user\", \"content\": augmented_query}\\n            ]\\n        )\\n\\n        bot_response= res.choices[0].message.content\\n\\n        #Update conversations\\n        self.conversation_history.append({\\'role\\':\\'user\\',\\'content\\':query})\\n        self.conversation_history.append({\\'role\\':\\'assistant\\',\\'content\\':bot_response})\\n\\n        return bot_response\\n    \\n    def reset_memory(self):\\n        \"\"\"\\n        Clear the conversation history.\\n        \"\"\"\\n        self.conversation_history = []\\n'),\n",
              " Document(metadata={'source': 'groq_utils/__init__.py', 'text': 'groq_utils/__init__.py\\n\\nfrom .GroqClass import GroqClass'}, page_content='groq_utils/__init__.py\\n\\nfrom .GroqClass import GroqClass')]"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "client = OpenAI(\n",
        "    base_url=\"https://openrouter.ai/api/v1\",\n",
        "    api_key=userdata.get(\"OPENROUTER_API_KEY\")\n",
        ")"
      ],
      "metadata": {
        "id": "3_Kpmt5Mbt8g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"How is the flask used?\"\n",
        "raw_query_embedding = get_huggingface_embeddings(query)"
      ],
      "metadata": {
        "id": "nw240c3KcBjS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_query_embedding"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BRBD7ImKcE6A",
        "outputId": "dc5dedf4-a04b-4d1e-ed6b-80b657cb6d17"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 5.71991131e-02, -3.48081291e-02, -3.27215418e-02,  5.29010817e-02,\n",
              "       -3.88054959e-02,  2.21033078e-02,  1.60938818e-02, -1.00735975e-02,\n",
              "        3.04608904e-02, -6.25874922e-02,  2.71743089e-02,  3.67565900e-02,\n",
              "        5.69119453e-02,  5.45354113e-02,  4.02487814e-02, -4.73744869e-02,\n",
              "        3.59910820e-03,  6.65521948e-03,  1.47536704e-02,  3.57538760e-02,\n",
              "        1.81228686e-02,  1.24748386e-02, -2.07926128e-02,  6.99328259e-02,\n",
              "       -1.78905539e-02, -1.98267922e-02, -8.77424423e-03, -4.04769043e-03,\n",
              "       -4.82026935e-02, -1.55368652e-02, -6.26485124e-02, -6.66373223e-03,\n",
              "        1.43068153e-02, -4.92968187e-02,  1.30648721e-06, -2.02893210e-03,\n",
              "       -4.47639599e-02,  2.07317546e-02, -2.80540297e-03,  1.37846796e-02,\n",
              "        4.11506603e-03,  6.87661488e-03, -2.91272085e-02, -6.68382505e-03,\n",
              "        2.94112526e-02, -4.13797721e-02,  3.90248671e-02, -5.73173016e-02,\n",
              "        3.29415090e-02,  1.95522374e-03, -7.05714687e-04, -2.74959207e-02,\n",
              "        8.47589783e-03,  5.72881177e-02,  1.15182819e-02, -2.74837762e-02,\n",
              "        9.83639061e-03, -3.90402637e-02,  2.65231058e-02,  2.21770331e-02,\n",
              "       -3.45997736e-02, -1.54186934e-02,  4.95924754e-03, -1.39193246e-02,\n",
              "        2.68028304e-03,  6.40870398e-03, -4.75133844e-02, -5.65078110e-02,\n",
              "        4.45124984e-04, -3.50493230e-02,  3.05134077e-02, -1.38917025e-02,\n",
              "        5.74368164e-02,  3.41166444e-02, -2.11743731e-02,  8.92364606e-02,\n",
              "       -2.50644702e-02, -4.00944464e-02, -2.53773164e-02,  2.24428754e-02,\n",
              "        2.05555819e-02, -1.37278633e-02, -4.36093891e-03, -3.98067497e-02,\n",
              "        1.22444024e-02,  6.88756406e-02, -1.91624984e-02,  1.02390936e-02,\n",
              "        2.36052368e-02,  3.67535241e-02, -3.08089629e-02, -9.60418060e-02,\n",
              "        3.46700586e-02,  7.60558015e-03,  9.76804271e-02, -1.58347022e-02,\n",
              "        6.36186078e-03, -4.67259996e-02,  1.72872264e-02, -3.31959799e-02,\n",
              "        1.39393723e-02,  5.40790111e-02, -6.83548599e-02,  2.75306888e-02,\n",
              "        2.55090147e-02,  5.65291941e-03,  4.33611162e-02, -3.92891839e-02,\n",
              "       -1.34463422e-03,  2.44514365e-02,  2.43255477e-02, -5.87054379e-02,\n",
              "       -2.45560892e-02,  8.56442470e-03, -1.87520776e-02,  3.01436987e-02,\n",
              "        3.30266990e-02,  2.43946873e-02,  1.19082564e-02,  3.93456295e-02,\n",
              "       -8.27320199e-03, -2.96888910e-02,  2.99564097e-02,  4.83968370e-02,\n",
              "       -3.96414399e-02,  5.54087311e-02, -5.52443974e-02, -4.09533456e-02,\n",
              "        2.51762141e-02, -7.45351091e-02,  2.15652455e-02,  6.81989919e-03,\n",
              "        2.51630731e-02, -2.66167130e-02, -7.87202176e-03,  2.03645229e-02,\n",
              "       -4.96322289e-02,  5.16786873e-02, -3.97936068e-03, -4.66504693e-02,\n",
              "       -1.27714081e-02,  5.39381765e-02,  6.07716627e-02, -5.20693325e-02,\n",
              "       -9.14145168e-03,  3.92798632e-02, -3.82390544e-02, -2.54382212e-02,\n",
              "       -1.19422507e-02,  2.77046878e-02, -2.28422638e-02,  1.93142500e-02,\n",
              "       -1.73437539e-02, -3.91559973e-02,  2.76857801e-03, -2.68673766e-02,\n",
              "       -4.06523468e-03,  2.71099471e-02,  5.50326379e-03, -8.65715183e-03,\n",
              "       -5.38523309e-03, -2.38758381e-02, -1.67614557e-02, -4.30299081e-02,\n",
              "       -2.02624667e-02, -1.52831962e-02, -3.04900017e-02, -2.98172352e-03,\n",
              "       -2.20250301e-02,  4.97563323e-03,  3.35392952e-02,  8.65502004e-03,\n",
              "       -3.52158844e-02, -2.41796877e-02, -5.46162017e-02,  6.46919459e-02,\n",
              "        3.09137329e-02,  6.81637824e-02,  6.54886588e-02,  3.34176458e-02,\n",
              "        7.92475268e-02,  6.95155486e-02, -2.44963896e-02, -2.03490388e-02,\n",
              "        4.01198715e-02,  2.72981985e-03, -7.68630356e-02,  7.44913220e-02,\n",
              "        4.25300933e-02, -3.43394279e-02,  7.25540216e-04,  5.14009818e-02,\n",
              "        2.04829704e-02, -1.03368303e-02, -3.25191095e-02, -9.89327300e-03,\n",
              "        3.73155028e-02,  1.45622957e-02, -3.11909313e-03,  1.36054363e-02,\n",
              "        2.61460710e-02, -3.69705968e-02, -7.29956552e-02,  6.29785433e-02,\n",
              "        4.84144129e-02,  2.19919141e-02, -1.09012667e-02,  3.46254325e-03,\n",
              "       -3.20377983e-02, -4.76485267e-02, -2.28098873e-03,  5.27650770e-03,\n",
              "       -5.92173003e-02,  7.27547379e-03, -1.52387209e-02,  3.88087449e-03,\n",
              "       -5.07637579e-03,  7.63851330e-02, -8.62326548e-02, -1.99151938e-04,\n",
              "       -2.91267354e-02, -3.06969285e-02,  3.16414014e-02,  2.48523126e-03,\n",
              "       -2.64980271e-02, -1.47979911e-02, -2.47980990e-02,  1.41937938e-02,\n",
              "       -4.73740362e-02,  3.34852899e-04, -2.39294395e-02,  1.47233473e-03,\n",
              "        1.51686436e-02,  1.65915210e-03,  6.51667565e-02,  3.18878926e-02,\n",
              "       -5.19753061e-02, -3.91487293e-02,  4.75128032e-02, -2.75391862e-02,\n",
              "       -2.13983115e-02,  2.49021351e-02,  2.59907432e-02, -8.86957441e-03,\n",
              "        2.82779820e-02, -3.33999172e-02,  1.11997565e-02,  5.82844391e-03,\n",
              "       -2.11793110e-02, -4.00041156e-02,  4.85287094e-03,  1.39900222e-02,\n",
              "        4.17894125e-03, -7.78923556e-03, -4.07452658e-02, -3.08802747e-03,\n",
              "        1.24430219e-02, -1.28500471e-02,  3.19465213e-02,  1.55629152e-02,\n",
              "        3.76610681e-02,  1.50284786e-02, -3.08945472e-03,  1.38458060e-02,\n",
              "       -3.01544759e-02,  7.57221738e-03, -3.40416506e-02, -5.21370471e-02,\n",
              "       -5.93046322e-02,  3.19491476e-02,  5.71038807e-03,  2.03530136e-02,\n",
              "        1.42997690e-02,  5.41717596e-02,  3.86603060e-03, -2.02749372e-02,\n",
              "        1.42188957e-02, -3.46890390e-02, -2.56205779e-02,  1.79553963e-02,\n",
              "        1.24495886e-02, -7.16773942e-02, -2.37138453e-03, -1.46819595e-02,\n",
              "       -5.17199859e-02,  4.68135327e-02, -1.01810284e-02, -2.03419961e-02,\n",
              "        3.34763601e-02,  4.86822166e-02,  2.60386113e-02,  8.65800083e-02,\n",
              "        2.55843587e-02, -1.48231238e-02,  1.54763730e-02, -2.24735308e-02,\n",
              "       -3.67613025e-02,  8.40102658e-02, -4.15653773e-02,  3.55002917e-02,\n",
              "       -2.45448556e-02, -5.36360256e-02, -3.72972004e-02,  1.25069078e-02,\n",
              "        3.31733413e-02, -1.56354103e-02, -6.76281452e-02, -7.84973875e-02,\n",
              "       -2.89759338e-02,  1.41797945e-01,  5.50606847e-02,  2.96505950e-02,\n",
              "       -1.88852102e-03,  2.57555721e-03, -2.04364732e-02,  3.05336528e-02,\n",
              "       -4.73619532e-03, -1.05110267e-02, -5.13510071e-02,  4.21647727e-02,\n",
              "        1.20367277e-02,  2.16573868e-02,  6.30189255e-02, -9.46695283e-02,\n",
              "       -2.55720410e-02, -3.23980600e-02,  3.68338898e-02,  4.76690708e-03,\n",
              "        1.15047703e-02,  1.00437272e-02, -2.41284966e-02, -3.10911331e-02,\n",
              "       -1.58535689e-02,  4.08831537e-02, -2.18220912e-02,  3.53736952e-02,\n",
              "       -2.94169020e-02, -4.26899754e-02,  7.75681483e-03,  8.99861101e-03,\n",
              "       -2.52819378e-02, -1.36864875e-02,  2.02667806e-02, -4.14673798e-02,\n",
              "        7.43439887e-03,  2.87979525e-02,  3.36821824e-02,  3.12989801e-02,\n",
              "       -2.69457046e-02, -1.73054133e-02,  3.86038050e-02,  3.35292108e-02,\n",
              "        1.68660711e-02, -2.47896067e-04, -6.19834326e-02, -5.70357293e-02,\n",
              "       -2.00222079e-02, -2.88821049e-02, -7.29324296e-03,  2.31045317e-02,\n",
              "        3.86906713e-02,  4.99564670e-02, -4.24653217e-02, -5.99629246e-02,\n",
              "       -4.41746227e-02, -1.52639868e-02,  5.46780489e-02, -1.19739389e-02,\n",
              "       -5.69641776e-03,  5.25579266e-02,  1.24874180e-02, -2.42425166e-02,\n",
              "       -2.83974223e-02,  1.83277465e-02, -7.52052292e-03, -2.72396039e-02,\n",
              "       -6.37565833e-03, -1.87853426e-02, -3.64199877e-02,  4.44277711e-02,\n",
              "       -4.84144548e-03, -2.59571001e-02, -2.72887591e-02,  5.13039194e-02,\n",
              "        4.55947854e-02, -1.83453914e-02, -9.67897847e-03,  4.89505343e-02,\n",
              "        1.05741043e-02,  2.34286338e-02, -1.59939250e-03,  1.21629843e-02,\n",
              "        1.74357519e-02,  8.69955420e-02, -2.49097049e-02,  3.48292887e-02,\n",
              "        7.69620994e-03,  3.07778567e-02,  6.87526632e-03,  4.10034023e-02,\n",
              "        5.58804767e-03, -6.51644766e-02, -2.15925626e-03, -2.64191367e-02,\n",
              "       -1.73870847e-02, -3.32179256e-02,  7.04814717e-02,  5.56455599e-03,\n",
              "        1.39858108e-02,  2.12791506e-02,  1.75066441e-02, -4.26362120e-02,\n",
              "        1.15706418e-02, -2.65381970e-02, -2.20380276e-02,  1.49483699e-02,\n",
              "       -9.76180751e-03,  2.02453099e-02,  3.22943483e-03,  1.96187962e-02,\n",
              "        2.59395596e-03, -7.23197125e-03,  7.51051353e-03, -4.48049456e-02,\n",
              "        6.28838614e-02,  2.21251380e-02,  5.62744541e-03, -3.55769205e-03,\n",
              "       -1.79370958e-02,  7.44422302e-02,  1.67561416e-02, -7.53110349e-02,\n",
              "       -1.13453552e-01,  2.72909198e-02, -5.47077619e-02, -1.50530171e-02,\n",
              "       -1.25812963e-02,  4.01325598e-02, -5.40130809e-02,  3.55191971e-03,\n",
              "        3.12822424e-02, -1.87432691e-02, -3.57308611e-02,  5.76704694e-03,\n",
              "       -6.83805346e-02,  3.80889550e-02, -1.63622538e-03,  1.38717145e-02,\n",
              "        1.73450122e-03,  5.04001230e-02, -1.97166257e-04,  2.09847298e-02,\n",
              "       -2.53032111e-02,  1.00756595e-02, -5.74005255e-03, -3.91292982e-02,\n",
              "       -4.11761478e-02,  1.97189506e-02,  7.77617039e-04,  3.66704650e-02,\n",
              "       -2.75941435e-02,  1.59035921e-02, -1.70494076e-02, -1.63443610e-02,\n",
              "       -9.65072121e-03, -9.38471183e-02,  4.11776192e-02, -9.08073038e-02,\n",
              "       -2.68844273e-02, -4.85741254e-03,  4.88195904e-02,  4.30380553e-02,\n",
              "       -3.16761471e-02, -5.54440450e-03, -5.39558660e-03, -1.32693946e-02,\n",
              "       -2.58765593e-02, -5.53768016e-02,  1.50012951e-02,  1.90001950e-02,\n",
              "        2.13003922e-02,  4.67846803e-02,  1.58928297e-02,  7.79179260e-02,\n",
              "        5.40408492e-02, -4.86199837e-03, -6.92401733e-03, -1.13790100e-02,\n",
              "       -3.90905850e-02, -2.86201760e-02,  2.77783405e-02, -3.61350179e-03,\n",
              "        8.33226666e-02,  6.88894242e-02, -3.57933640e-02,  2.55738031e-02,\n",
              "       -4.98848082e-03,  7.69988000e-02,  1.79184191e-02, -4.02382575e-02,\n",
              "        5.87942544e-03,  1.80999599e-02, -4.69399523e-03, -2.60235686e-02,\n",
              "        4.01100107e-02, -1.45588315e-03, -4.86016460e-02,  1.99401472e-02,\n",
              "        4.31803800e-03,  7.88210798e-03,  3.08645014e-02, -1.94113683e-02,\n",
              "       -2.27649808e-02,  2.96580996e-02,  6.04185015e-02, -3.13249528e-02,\n",
              "        2.39116941e-02, -7.98480026e-03,  4.28687260e-02,  2.43713427e-02,\n",
              "        6.16064575e-03,  9.98261385e-03, -2.26695333e-02, -5.75594418e-03,\n",
              "        2.52199657e-02, -3.63932997e-02, -2.26824116e-02, -3.48698646e-02,\n",
              "       -1.28334031e-01,  5.05821928e-02,  1.45999258e-02, -4.99502048e-02,\n",
              "       -1.16854310e-02, -5.10387644e-02, -4.39276407e-03, -1.76934954e-02,\n",
              "        1.85698103e-02,  4.24964577e-02,  3.62612517e-03, -8.21787491e-03,\n",
              "        1.52608445e-02, -3.99135798e-02, -1.72168817e-02, -2.55729314e-02,\n",
              "        2.00702455e-02, -2.86136828e-02,  5.72780706e-03,  3.12902592e-02,\n",
              "        4.64497730e-02, -3.95247228e-02, -3.20839658e-02,  9.26095843e-02,\n",
              "        2.89518703e-02, -4.08141920e-03, -1.50173055e-02, -4.41565012e-33,\n",
              "       -4.74333391e-02,  3.32583189e-02,  1.03685381e-02,  5.93765788e-02,\n",
              "        1.88487619e-02, -7.81426951e-02, -2.78754104e-02, -5.63828312e-02,\n",
              "        1.78628378e-02, -1.60493776e-02,  4.21900675e-02,  8.91905744e-03,\n",
              "        1.28873170e-03,  2.24172082e-02,  1.51028158e-02, -5.00620417e-02,\n",
              "        1.40005089e-02, -5.46086160e-03, -2.26302817e-02, -6.18816689e-02,\n",
              "        5.18094487e-02,  1.19468691e-02, -1.44731235e-02,  3.07529178e-02,\n",
              "       -2.58049890e-02, -9.86220874e-03,  2.09785393e-03,  8.37041531e-03,\n",
              "        6.02251180e-02,  6.98747709e-02,  2.91903857e-02, -2.45108344e-02,\n",
              "       -7.29592564e-03, -2.38358825e-02, -1.74369421e-02,  6.75049201e-02,\n",
              "       -1.03483006e-01, -2.44491789e-02,  5.89062497e-02,  8.48937780e-02,\n",
              "       -2.22708918e-02, -2.91215051e-02,  6.03993908e-02, -1.43617103e-02,\n",
              "       -4.30093408e-02, -3.15463059e-02, -2.00793147e-02, -2.32756212e-02,\n",
              "        5.45023754e-03, -2.29356028e-02,  2.18587089e-02,  1.31977256e-02,\n",
              "       -5.85779324e-02,  3.66184227e-02,  8.90511274e-02,  7.60766789e-02,\n",
              "        2.29219161e-02, -8.89664236e-03,  2.57415250e-02, -3.64604443e-02,\n",
              "       -3.63916866e-02,  3.28687914e-02,  1.30067132e-02,  2.96794567e-02,\n",
              "        4.47977241e-03,  2.02063043e-02,  2.59186665e-04, -4.30257842e-02,\n",
              "       -2.88880952e-02,  8.59236270e-02, -8.61762092e-04, -9.64332465e-03,\n",
              "       -2.20969543e-02, -1.97867006e-02, -4.89864219e-03, -3.50129697e-03,\n",
              "       -4.05743672e-03,  2.54365280e-02, -2.47489265e-03,  6.20073229e-02,\n",
              "       -1.31220398e-02,  4.32156632e-03,  3.89345400e-02, -2.68731900e-02,\n",
              "       -2.84163542e-02, -2.78223753e-02, -2.92000175e-02, -2.83088572e-02,\n",
              "       -2.04930920e-02, -3.39918807e-02, -8.12643557e-04,  4.55642343e-02,\n",
              "       -1.17630716e-02, -4.73280139e-02,  4.34421673e-02,  2.05987338e-02,\n",
              "       -5.64046577e-02,  1.54250693e-02,  5.22709498e-03, -1.38512496e-02,\n",
              "        8.95439647e-03, -2.38714553e-02, -5.65449893e-02, -5.69092631e-02,\n",
              "        4.25568409e-02,  1.24976775e-02, -2.92897727e-02, -3.88705954e-02,\n",
              "       -4.52911370e-02, -4.93761734e-04, -2.76485737e-02,  4.34354581e-02,\n",
              "       -1.40210036e-02, -1.12787308e-02,  4.59000990e-02,  1.39154075e-02,\n",
              "       -2.20193826e-02, -5.88837592e-03, -6.79737516e-03, -7.00293761e-03,\n",
              "        5.42924274e-04, -4.00976241e-02, -1.60106067e-02,  1.95955653e-02,\n",
              "       -5.37425019e-02, -3.62902172e-02,  2.77884919e-02,  6.25473168e-03,\n",
              "        3.46342064e-02,  3.21694314e-02,  8.12013634e-03,  2.52433438e-02,\n",
              "        1.90433298e-07,  4.38987985e-02,  2.80597527e-02,  2.45935358e-02,\n",
              "       -1.02043822e-02,  6.41568378e-02,  1.43293105e-02, -1.70972012e-02,\n",
              "        3.34620811e-02,  3.23122367e-02, -5.29283993e-02, -5.55978827e-02,\n",
              "       -2.66577434e-02,  3.09817977e-02,  5.11521772e-02, -4.48862389e-02,\n",
              "        1.54932030e-02, -2.53988262e-02, -2.83563472e-02,  1.72856252e-03,\n",
              "        9.29574966e-02,  1.55642899e-02,  2.24222075e-02, -1.63166504e-02,\n",
              "        5.49935643e-03,  3.24896388e-02, -5.44768050e-02, -6.08727382e-03,\n",
              "        6.90511148e-03, -5.46010351e-03,  1.10500483e-02, -5.91143481e-02,\n",
              "       -5.12277521e-03,  1.05564753e-02, -3.12648481e-03,  2.27596075e-03,\n",
              "       -1.17113227e-02,  7.53145525e-03,  8.07938427e-02,  2.84501240e-02,\n",
              "        9.90494117e-02,  2.89266426e-02, -1.05028056e-01, -1.28771700e-02,\n",
              "        1.14156175e-02,  2.62483805e-02, -5.94671303e-03, -3.32753882e-02,\n",
              "       -1.81747675e-02, -1.53868729e-02, -5.29382192e-02, -6.13175407e-02,\n",
              "       -7.93049298e-03, -3.12193646e-03, -2.62025241e-02, -1.39061101e-02,\n",
              "       -4.54897135e-02,  2.37891916e-02,  9.08095762e-02, -2.70755142e-02,\n",
              "        6.52734190e-02, -4.87189144e-02, -2.26315837e-02, -3.94732170e-02,\n",
              "        1.39855575e-02,  1.24080165e-03,  8.30419809e-02, -1.35176163e-02,\n",
              "        1.63212477e-34,  1.51399104e-02, -1.50091005e-02, -1.88204721e-02,\n",
              "       -2.25235280e-02,  1.63258407e-02, -4.47282335e-03, -2.58178059e-02,\n",
              "        3.47461849e-02, -7.84784369e-03, -1.07982252e-02, -5.36752772e-03],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "top_matches = pinecone_index.query(vector=raw_query_embedding.tolist(),\n",
        "                                   top_k=5,\n",
        "                                   include_metadata=True,\n",
        "                                   namespace=github_repo)"
      ],
      "metadata": {
        "id": "ZAn3CpAmcIL1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "top_matches"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "c_TC_Zj4cJIh",
        "outputId": "be26d764-ee95-465f-b7f5-0c9bd69d9d4b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'matches': [{'id': 'dfb6a3d0-a630-4455-8a60-0b589003e2a6',\n",
              "              'metadata': {'source': 'groq_utils/__init__.py',\n",
              "                           'text': 'groq_utils/__init__.py\\n'\n",
              "                                   '\\n'\n",
              "                                   'from .GroqClass import GroqClass'},\n",
              "              'score': 0.188397408,\n",
              "              'values': []},\n",
              "             {'id': '17048fbe-be8e-4121-a2ad-ebf339c823dd',\n",
              "              'metadata': {'source': 'groq_utils/__init__.py',\n",
              "                           'text': 'groq_utils/__init__.py\\n'\n",
              "                                   '\\n'\n",
              "                                   'from .GroqClass import GroqClass'},\n",
              "              'score': 0.188397408,\n",
              "              'values': []},\n",
              "             {'id': '2173527e-7217-49cd-b7c5-776fa1049735',\n",
              "              'metadata': {'source': 'app.py',\n",
              "                           'text': 'app.py\\n'\n",
              "                                   '\\n'\n",
              "                                   'from flask import Flask, render_template, '\n",
              "                                   'request, jsonify\\n'\n",
              "                                   'from groq_utils import GroqClass\\n'\n",
              "                                   'from preprocessing import '\n",
              "                                   'DocumentProcessor\\n'\n",
              "                                   'from transcriber import '\n",
              "                                   'YouTubeTranscriber\\n'\n",
              "                                   'from langchain_community.embeddings import '\n",
              "                                   'HuggingFaceEmbeddings\\n'\n",
              "                                   'import os\\n'\n",
              "                                   'from dotenv import load_dotenv\\n'\n",
              "                                   'app = Flask(__name__)\\n'\n",
              "                                   '# Load environment variables from .env '\n",
              "                                   'file\\n'\n",
              "                                   'load_dotenv()\\n'\n",
              "                                   '\\n'\n",
              "                                   'index_name = \"insightbot\"\\n'\n",
              "                                   'namespace = \"transcripts\"\\n'\n",
              "                                   '\\n'\n",
              "                                   '# Initialize embeddings and shared '\n",
              "                                   'classes\\n'\n",
              "                                   'embeddings = '\n",
              "                                   'HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\\n'\n",
              "                                   'groq_client = GroqClass()  # Initialize '\n",
              "                                   'GroqClass globally to be reused\\n'\n",
              "                                   'document_processor = '\n",
              "                                   'DocumentProcessor(index_name=index_name,namespace=namespace)  '\n",
              "                                   '# Initialize DocumentProcessor globpally\\n'\n",
              "                                   '# GLOBAL\\n'\n",
              "                                   'document_dir_path = \"resources/documents\"\\n'\n",
              "                                   'transcript_dir_path = '\n",
              "                                   '\"resources/transcripts\"\\n'\n",
              "                                   '\\n'\n",
              "                                   \"@app.route('/')\\n\"\n",
              "                                   'def home():\\n'\n",
              "                                   \"    return render_template('index.html')\\n\"\n",
              "                                   '\\n'\n",
              "                                   \"@app.route('/chatbot')\\n\"\n",
              "                                   'def chatbot():\\n'\n",
              "                                   '    return '\n",
              "                                   \"render_template('chatbot.html')\\n\"\n",
              "                                   '\\n'\n",
              "                                   \"@app.route('/submit_media', \"\n",
              "                                   \"methods=['POST'])\\n\"\n",
              "                                   'def submit_media():\\n'\n",
              "                                   '    try:\\n'\n",
              "                                   '        groq_client.reset_memory()\\n'\n",
              "                                   '        '\n",
              "                                   'document_processor.delete_files_in_directory(transcript_dir_path)\\n'\n",
              "                                   '        '\n",
              "                                   'document_processor.delete_files_in_directory(document_dir_path)\\n'\n",
              "                                   '\\n'\n",
              "                                   '        # Get YouTube links and documents '\n",
              "                                   'from the request\\n'\n",
              "                                   '        youtube_links = '\n",
              "                                   \"request.form.getlist('youtube_links[]')  # \"\n",
              "                                   'List of YouTube links\\n'\n",
              "                                   '        files = '\n",
              "                                   \"request.files.getlist('documents[]')  # \"\n",
              "                                   'List of uploaded files\\n'\n",
              "                                   '\\n'\n",
              "                                   '        # Validate input\\n'\n",
              "                                   '        if not youtube_links and not '\n",
              "                                   'files:\\n'\n",
              "                                   '            return jsonify({\"error\": \"At '\n",
              "                                   'least one YouTube link or document is '\n",
              "                                   'required\"}), 400\\n'\n",
              "                                   '\\n'\n",
              "                                   '        all_documents = []\\n'\n",
              "                                   '\\n'\n",
              "                                   '        # Process each YouTube link\\n'\n",
              "                                   '        if youtube_links:\\n'\n",
              "                                   '            print(\"=== Processing YouTube '\n",
              "                                   'Links ===\")\\n'\n",
              "                                   '            for youtube_link in '\n",
              "                                   'youtube_links:\\n'\n",
              "                                   '                if youtube_link:\\n'\n",
              "                                   '                    print(f\"Processing '\n",
              "                                   'YouTube Link: {youtube_link}\")\\n'\n",
              "                                   '                    transcriber = '\n",
              "                                   'YouTubeTranscriber(youtube_link, '\n",
              "                                   'output_dir = transcript_dir_path)\\n'\n",
              "                                   '                    '\n",
              "                                   'transcriber.transcribe()\\n'\n",
              "                                   '                    print(f\"Transcription '\n",
              "                                   'complete for: {youtube_link}\")\\n'\n",
              "                                   '            documents = '\n",
              "                                   'document_processor.process_directory(transcript_dir_path)\\n'\n",
              "                                   '            '\n",
              "                                   'all_documents.extend(documents)\\n'\n",
              "                                   '\\n'\n",
              "                                   '        # Process uploaded documents\\n'\n",
              "                                   '        if files:\\n'\n",
              "                                   '            print(\"=== Processing Uploaded '\n",
              "                                   'Documents ===\")\\n'\n",
              "                                   '            for file in files:\\n'\n",
              "                                   '                file_path = '\n",
              "                                   'os.path.join(document_dir_path, '\n",
              "                                   'file.filename)\\n'\n",
              "                                   '                file.save(file_path)\\n'\n",
              "                                   '                print(f\"Saved file: '\n",
              "                                   '{file_path}\")\\n'\n",
              "                                   '            uploaded_documents = '\n",
              "                                   'document_processor.process_directory(document_dir_path)\\n'\n",
              "                                   '            '\n",
              "                                   'all_documents.extend(uploaded_documents)\\n'\n",
              "                                   '\\n'\n",
              "                                   '        if not all_documents:\\n'\n",
              "                                   '            return jsonify({\"error\": \"No '\n",
              "                                   'valid documents to process\"}), 400\\n'\n",
              "                                   '\\n'\n",
              "                                   '        # Prepare and chunk data\\n'\n",
              "                                   '        document_data = '\n",
              "                                   'document_processor.prepare_data(all_documents)\\n'\n",
              "                                   '        documents_chunks = '\n",
              "                                   'document_processor.chunk_data(document_data)\\n'\n",
              "                                   '\\n'\n",
              "                                   '        vectorstore_from_documents = '\n",
              "                                   'document_processor.upsert_vectorstore_to_pinecone(documents_chunks, '\n",
              "                                   'embeddings, index_name, namespace)\\n'\n",
              "                                   \"        print('=== Upsert to Pinecone done \"\n",
              "                                   \"===', vectorstore_from_documents)\\n\"\n",
              "                                   '\\n'\n",
              "                                   '        return jsonify({\"message\": \"Media '\n",
              "                                   'processed successfully!\"})\\n'\n",
              "                                   '\\n'\n",
              "                                   '    except Exception as e:\\n'\n",
              "                                   '        print(f\"Error: {str(e)}\")\\n'\n",
              "                                   '        return jsonify({\"error\": \"Failed '\n",
              "                                   'to process media\", \"details\": str(e)}), '\n",
              "                                   '500\\n'\n",
              "                                   '\\n'\n",
              "                                   \"@app.route('/submit_youtube_link', \"\n",
              "                                   \"methods=['POST'])\\n\"\n",
              "                                   '\\n'\n",
              "                                   \"@app.route('/ask_question', \"\n",
              "                                   \"methods=['POST'])\\n\"\n",
              "                                   'def ask_question():\\n'\n",
              "                                   '    question = '\n",
              "                                   \"request.form.get('question')\\n\"\n",
              "                                   '    # Get the response from the model\\n'\n",
              "                                   '    pinecone_index = '\n",
              "                                   'groq_client.initialize_pinecone(index_name)\\n'\n",
              "                                   '    '\n",
              "                                   'answer=groq_client.perform_rag(pinecone_index, '\n",
              "                                   'namespace, question)\\n'\n",
              "                                   '    return jsonify({\"question\": question, '\n",
              "                                   '\"answer\": answer})\\n'\n",
              "                                   '\\n'\n",
              "                                   '# Simulating model response for now\\n'\n",
              "                                   'def get_model_response(question):\\n'\n",
              "                                   '    # Replace with actual logic to handle '\n",
              "                                   'YouTube video and query\\n'\n",
              "                                   '    return f\"Answer to your question '\n",
              "                                   '\\'{question}\\'\"\\n'\n",
              "                                   '\\n'\n",
              "                                   '\\n'\n",
              "                                   '# if __name__ == \"__main__\":\\n'\n",
              "                                   '#     app.run(debug=False)\\n'\n",
              "                                   'if __name__==\"__main__\":\\n'\n",
              "                                   \"    app.run(host=os.getenv('IP', \"\n",
              "                                   \"'0.0.0.0'), \\n\"\n",
              "                                   \"            port=int(os.getenv('PORT', \"\n",
              "                                   '4444)))'},\n",
              "              'score': 0.168163776,\n",
              "              'values': []},\n",
              "             {'id': '07aa2071-c1a0-48d5-ab0e-deacce2a3aac',\n",
              "              'metadata': {'source': 'app.py',\n",
              "                           'text': 'app.py\\n'\n",
              "                                   '\\n'\n",
              "                                   'from flask import Flask, render_template, '\n",
              "                                   'request, jsonify\\n'\n",
              "                                   'from groq_utils import GroqClass\\n'\n",
              "                                   'from preprocessing import '\n",
              "                                   'DocumentProcessor\\n'\n",
              "                                   'from transcriber import '\n",
              "                                   'YouTubeTranscriber\\n'\n",
              "                                   'from langchain_community.embeddings import '\n",
              "                                   'HuggingFaceEmbeddings\\n'\n",
              "                                   'import os\\n'\n",
              "                                   'from dotenv import load_dotenv\\n'\n",
              "                                   'app = Flask(__name__)\\n'\n",
              "                                   '# Load environment variables from .env '\n",
              "                                   'file\\n'\n",
              "                                   'load_dotenv()\\n'\n",
              "                                   '\\n'\n",
              "                                   'index_name = \"insightbot\"\\n'\n",
              "                                   'namespace = \"transcripts\"\\n'\n",
              "                                   '\\n'\n",
              "                                   '# Initialize embeddings and shared '\n",
              "                                   'classes\\n'\n",
              "                                   'embeddings = '\n",
              "                                   'HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\\n'\n",
              "                                   'groq_client = GroqClass()  # Initialize '\n",
              "                                   'GroqClass globally to be reused\\n'\n",
              "                                   'document_processor = '\n",
              "                                   'DocumentProcessor(index_name=index_name,namespace=namespace)  '\n",
              "                                   '# Initialize DocumentProcessor globpally\\n'\n",
              "                                   '# GLOBAL\\n'\n",
              "                                   'document_dir_path = \"resources/documents\"\\n'\n",
              "                                   'transcript_dir_path = '\n",
              "                                   '\"resources/transcripts\"\\n'\n",
              "                                   '\\n'\n",
              "                                   \"@app.route('/')\\n\"\n",
              "                                   'def home():\\n'\n",
              "                                   \"    return render_template('index.html')\\n\"\n",
              "                                   '\\n'\n",
              "                                   \"@app.route('/chatbot')\\n\"\n",
              "                                   'def chatbot():\\n'\n",
              "                                   '    return '\n",
              "                                   \"render_template('chatbot.html')\\n\"\n",
              "                                   '\\n'\n",
              "                                   \"@app.route('/submit_media', \"\n",
              "                                   \"methods=['POST'])\\n\"\n",
              "                                   'def submit_media():\\n'\n",
              "                                   '    try:\\n'\n",
              "                                   '        groq_client.reset_memory()\\n'\n",
              "                                   '        '\n",
              "                                   'document_processor.delete_files_in_directory(transcript_dir_path)\\n'\n",
              "                                   '        '\n",
              "                                   'document_processor.delete_files_in_directory(document_dir_path)\\n'\n",
              "                                   '\\n'\n",
              "                                   '        # Get YouTube links and documents '\n",
              "                                   'from the request\\n'\n",
              "                                   '        youtube_links = '\n",
              "                                   \"request.form.getlist('youtube_links[]')  # \"\n",
              "                                   'List of YouTube links\\n'\n",
              "                                   '        files = '\n",
              "                                   \"request.files.getlist('documents[]')  # \"\n",
              "                                   'List of uploaded files\\n'\n",
              "                                   '\\n'\n",
              "                                   '        # Validate input\\n'\n",
              "                                   '        if not youtube_links and not '\n",
              "                                   'files:\\n'\n",
              "                                   '            return jsonify({\"error\": \"At '\n",
              "                                   'least one YouTube link or document is '\n",
              "                                   'required\"}), 400\\n'\n",
              "                                   '\\n'\n",
              "                                   '        all_documents = []\\n'\n",
              "                                   '\\n'\n",
              "                                   '        # Process each YouTube link\\n'\n",
              "                                   '        if youtube_links:\\n'\n",
              "                                   '            print(\"=== Processing YouTube '\n",
              "                                   'Links ===\")\\n'\n",
              "                                   '            for youtube_link in '\n",
              "                                   'youtube_links:\\n'\n",
              "                                   '                if youtube_link:\\n'\n",
              "                                   '                    print(f\"Processing '\n",
              "                                   'YouTube Link: {youtube_link}\")\\n'\n",
              "                                   '                    transcriber = '\n",
              "                                   'YouTubeTranscriber(youtube_link, '\n",
              "                                   'output_dir = transcript_dir_path)\\n'\n",
              "                                   '                    '\n",
              "                                   'transcriber.transcribe()\\n'\n",
              "                                   '                    print(f\"Transcription '\n",
              "                                   'complete for: {youtube_link}\")\\n'\n",
              "                                   '            documents = '\n",
              "                                   'document_processor.process_directory(transcript_dir_path)\\n'\n",
              "                                   '            '\n",
              "                                   'all_documents.extend(documents)\\n'\n",
              "                                   '\\n'\n",
              "                                   '        # Process uploaded documents\\n'\n",
              "                                   '        if files:\\n'\n",
              "                                   '            print(\"=== Processing Uploaded '\n",
              "                                   'Documents ===\")\\n'\n",
              "                                   '            for file in files:\\n'\n",
              "                                   '                file_path = '\n",
              "                                   'os.path.join(document_dir_path, '\n",
              "                                   'file.filename)\\n'\n",
              "                                   '                file.save(file_path)\\n'\n",
              "                                   '                print(f\"Saved file: '\n",
              "                                   '{file_path}\")\\n'\n",
              "                                   '            uploaded_documents = '\n",
              "                                   'document_processor.process_directory(document_dir_path)\\n'\n",
              "                                   '            '\n",
              "                                   'all_documents.extend(uploaded_documents)\\n'\n",
              "                                   '\\n'\n",
              "                                   '        if not all_documents:\\n'\n",
              "                                   '            return jsonify({\"error\": \"No '\n",
              "                                   'valid documents to process\"}), 400\\n'\n",
              "                                   '\\n'\n",
              "                                   '        # Prepare and chunk data\\n'\n",
              "                                   '        document_data = '\n",
              "                                   'document_processor.prepare_data(all_documents)\\n'\n",
              "                                   '        documents_chunks = '\n",
              "                                   'document_processor.chunk_data(document_data)\\n'\n",
              "                                   '\\n'\n",
              "                                   '        vectorstore_from_documents = '\n",
              "                                   'document_processor.upsert_vectorstore_to_pinecone(documents_chunks, '\n",
              "                                   'embeddings, index_name, namespace)\\n'\n",
              "                                   \"        print('=== Upsert to Pinecone done \"\n",
              "                                   \"===', vectorstore_from_documents)\\n\"\n",
              "                                   '\\n'\n",
              "                                   '        return jsonify({\"message\": \"Media '\n",
              "                                   'processed successfully!\"})\\n'\n",
              "                                   '\\n'\n",
              "                                   '    except Exception as e:\\n'\n",
              "                                   '        print(f\"Error: {str(e)}\")\\n'\n",
              "                                   '        return jsonify({\"error\": \"Failed '\n",
              "                                   'to process media\", \"details\": str(e)}), '\n",
              "                                   '500\\n'\n",
              "                                   '\\n'\n",
              "                                   \"@app.route('/submit_youtube_link', \"\n",
              "                                   \"methods=['POST'])\\n\"\n",
              "                                   '\\n'\n",
              "                                   \"@app.route('/ask_question', \"\n",
              "                                   \"methods=['POST'])\\n\"\n",
              "                                   'def ask_question():\\n'\n",
              "                                   '    question = '\n",
              "                                   \"request.form.get('question')\\n\"\n",
              "                                   '    # Get the response from the model\\n'\n",
              "                                   '    pinecone_index = '\n",
              "                                   'groq_client.initialize_pinecone(index_name)\\n'\n",
              "                                   '    '\n",
              "                                   'answer=groq_client.perform_rag(pinecone_index, '\n",
              "                                   'namespace, question)\\n'\n",
              "                                   '    return jsonify({\"question\": question, '\n",
              "                                   '\"answer\": answer})\\n'\n",
              "                                   '\\n'\n",
              "                                   '# Simulating model response for now\\n'\n",
              "                                   'def get_model_response(question):\\n'\n",
              "                                   '    # Replace with actual logic to handle '\n",
              "                                   'YouTube video and query\\n'\n",
              "                                   '    return f\"Answer to your question '\n",
              "                                   '\\'{question}\\'\"\\n'\n",
              "                                   '\\n'\n",
              "                                   '\\n'\n",
              "                                   '# if __name__ == \"__main__\":\\n'\n",
              "                                   '#     app.run(debug=False)\\n'\n",
              "                                   'if __name__==\"__main__\":\\n'\n",
              "                                   \"    app.run(host=os.getenv('IP', \"\n",
              "                                   \"'0.0.0.0'), \\n\"\n",
              "                                   \"            port=int(os.getenv('PORT', \"\n",
              "                                   '4444)))'},\n",
              "              'score': 0.168163776,\n",
              "              'values': []},\n",
              "             {'id': '0282a095-31e6-4a79-8a31-152468b89fad',\n",
              "              'metadata': {'source': 'preprocessing/__init__.py',\n",
              "                           'text': 'preprocessing/__init__.py\\n'\n",
              "                                   '\\n'\n",
              "                                   'from .Prepare import DocumentProcessor'},\n",
              "              'score': 0.12170849,\n",
              "              'values': []}],\n",
              " 'namespace': 'https://github.com/aneeshkrishna4739/InSightBot',\n",
              " 'usage': {'read_units': 6}}"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "context = [item['metadata']['text'] for item in top_matches['matches']]"
      ],
      "metadata": {
        "id": "ERQuL4cycN9o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "context"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dzitkWMLcPh6",
        "outputId": "9aa4f516-cd9d-4258-c2e5-6cda938a00ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['src/context/language/javascript-parser.ts\\n\\nimport { AbstractParser, EnclosingContext } from \"../../constants\";\\nimport * as parser from \"@babel/parser\";\\nimport traverse, { NodePath, Node } from \"@babel/traverse\";\\n\\nconst processNode = (\\n  path: NodePath<Node>,\\n  lineStart: number,\\n  lineEnd: number,\\n  largestSize: number,\\n  largestEnclosingContext: Node | null\\n) => {\\n  const { start, end } = path.node.loc;\\n  if (start.line <= lineStart && lineEnd <= end.line) {\\n    const size = end.line - start.line;\\n    if (size > largestSize) {\\n      largestSize = size;\\n      largestEnclosingContext = path.node;\\n    }\\n  }\\n  return { largestSize, largestEnclosingContext };\\n};\\n\\nexport class JavascriptParser implements AbstractParser {\\n  findEnclosingContext(\\n    file: string,\\n    lineStart: number,\\n    lineEnd: number\\n  ): EnclosingContext {\\n    const ast = parser.parse(file, {\\n      sourceType: \"module\",\\n      plugins: [\"jsx\", \"typescript\"], // To allow JSX and TypeScript\\n    });\\n    let largestEnclosingContext: Node = null;\\n    let largestSize = 0;\\n    traverse(ast, {\\n      Function(path) {\\n        ({ largestSize, largestEnclosingContext } = processNode(\\n          path,\\n          lineStart,\\n          lineEnd,\\n          largestSize,\\n          largestEnclosingContext\\n        ));\\n      },\\n      TSInterfaceDeclaration(path) {\\n        ({ largestSize, largestEnclosingContext } = processNode(\\n          path,\\n          lineStart,\\n          lineEnd,\\n          largestSize,\\n          largestEnclosingContext\\n        ));\\n      },\\n    });\\n    return {\\n      enclosingContext: largestEnclosingContext,\\n    } as EnclosingContext;\\n  }\\n\\n  dryRun(file: string): { valid: boolean; error: string } {\\n    try {\\n      const ast = parser.parse(file, {\\n        sourceType: \"module\",\\n        plugins: [\"jsx\", \"typescript\"], // To allow JSX and TypeScript\\n      });\\n      return {\\n        valid: true,\\n        error: \"\",\\n      };\\n    } catch (exc) {\\n      return {\\n        valid: false,\\n        error: exc,\\n      };\\n    }\\n  }\\n}\\n',\n",
              " 'src/context/language/python-parser.ts\\n\\nimport { AbstractParser, EnclosingContext } from \"../../constants\";\\nexport class PythonParser implements AbstractParser {\\n  findEnclosingContext(\\n    file: string,\\n    lineStart: number,\\n    lineEnd: number\\n  ): EnclosingContext {\\n    // TODO: Implement this method for Python\\n    return null;\\n  }\\n  dryRun(file: string): { valid: boolean; error: string } {\\n    // TODO: Implement this method for Python\\n    return { valid: false, error: \"Not implemented yet\" };\\n  }\\n}\\n',\n",
              " 'src/context/review.ts\\n\\nimport {\\n  AbstractParser,\\n  PRFile,\\n  PatchInfo,\\n  getParserForExtension,\\n} from \"../constants\";\\nimport * as diff from \"diff\";\\nimport { JavascriptParser } from \"./language/javascript-parser\";\\nimport { Node } from \"@babel/traverse\";\\n\\nconst expandHunk = (\\n  contents: string,\\n  hunk: diff.Hunk,\\n  linesAbove: number = 5,\\n  linesBelow: number = 5\\n) => {\\n  const fileLines = contents.split(\"\\\\n\");\\n  const curExpansion: string[] = [];\\n  const start = Math.max(0, hunk.oldStart - 1 - linesAbove);\\n  const end = Math.min(\\n    fileLines.length,\\n    hunk.oldStart - 1 + hunk.oldLines + linesBelow\\n  );\\n\\n  for (let i = start; i < hunk.oldStart - 1; i++) {\\n    curExpansion.push(fileLines[i]);\\n  }\\n\\n  curExpansion.push(\\n    `@@ -${hunk.oldStart},${hunk.oldLines} +${hunk.newStart},${hunk.newLines} @@`\\n  );\\n  hunk.lines.forEach((line) => {\\n    if (!curExpansion.includes(line)) {\\n      curExpansion.push(line);\\n    }\\n  });\\n\\n  for (let i = hunk.oldStart - 1 + hunk.oldLines; i < end; i++) {\\n    curExpansion.push(fileLines[i]);\\n  }\\n  return curExpansion.join(\"\\\\n\");\\n};\\n\\nconst expandFileLines = (\\n  file: PRFile,\\n  linesAbove: number = 5,\\n  linesBelow: number = 5\\n) => {\\n  const fileLines = file.old_contents.split(\"\\\\n\");\\n  const patches: PatchInfo[] = diff.parsePatch(file.patch);\\n  const expandedLines: string[][] = [];\\n  patches.forEach((patch) => {\\n    patch.hunks.forEach((hunk) => {\\n      const curExpansion: string[] = [];\\n      const start = Math.max(0, hunk.oldStart - 1 - linesAbove);\\n      const end = Math.min(\\n        fileLines.length,\\n        hunk.oldStart - 1 + hunk.oldLines + linesBelow\\n      );\\n\\n      for (let i = start; i < hunk.oldStart - 1; i++) {\\n        curExpansion.push(fileLines[i]);\\n      }\\n\\n      curExpansion.push(\\n        `@@ -${hunk.oldStart},${hunk.oldLines} +${hunk.newStart},${hunk.newLines} @@`\\n      );\\n      hunk.lines.forEach((line) => {\\n        if (!curExpansion.includes(line)) {\\n          curExpansion.push(line);\\n        }\\n      });\\n\\n      for (let i = hunk.oldStart - 1 + hunk.oldLines; i < end; i++) {\\n        curExpansion.push(fileLines[i]);\\n      }\\n      expandedLines.push(curExpansion);\\n    });\\n  });\\n\\n  return expandedLines;\\n};\\n\\nexport const expandedPatchStrategy = (file: PRFile) => {\\n  const expandedPatches = expandFileLines(file);\\n  const expansions = expandedPatches\\n    .map((patchLines) => patchLines.join(\"\\\\n\"))\\n    .join(\"\\\\n\\\\n\");\\n  return `## ${file.filename}\\\\n\\\\n${expansions}`;\\n};\\n\\nexport const rawPatchStrategy = (file: PRFile) => {\\n  return `## ${file.filename}\\\\n\\\\n${file.patch}`;\\n};\\n\\nconst trimHunk = (hunk: diff.Hunk): diff.Hunk => {\\n  const startIdx = hunk.lines.findIndex(\\n    (line) => line.startsWith(\"+\") || line.startsWith(\"-\")\\n  );\\n  const endIdx = hunk.lines\\n    .slice()\\n    .reverse()\\n    .findIndex((line) => line.startsWith(\"+\") || line.startsWith(\"-\"));\\n  const editLines = hunk.lines.slice(startIdx, hunk.lines.length - endIdx);\\n  return { ...hunk, lines: editLines, newStart: startIdx + hunk.newStart };\\n};\\n\\nconst buildingScopeString = (\\n  currentFile: string,\\n  scope: Node,\\n  hunk: diff.Hunk\\n) => {\\n  const res: string[] = [];\\n  const trimmedHunk = trimHunk(hunk);\\n  const functionStartLine = scope.loc.start.line;\\n  const functionEndLine = scope.loc.end.line;\\n  const updatedFileLines = currentFile.split(\"\\\\n\");\\n  // Extract the lines of the function\\n  const functionContext = updatedFileLines.slice(\\n    functionStartLine - 1,\\n    functionEndLine\\n  );\\n  // Calculate the index where the changes should be injected into the function\\n  const injectionIdx =\\n    hunk.newStart -\\n    functionStartLine +\\n    hunk.lines.findIndex(\\n      (line) => line.startsWith(\"+\") || line.startsWith(\"-\")\\n    );\\n  // Count the number of lines that should be dropped from the function\\n  const dropCount = trimmedHunk.lines.filter(\\n    (line) => !line.startsWith(\"-\")\\n  ).length;\\n\\n  const hunkHeader = `@@ -${hunk.oldStart},${hunk.oldLines} +${hunk.newStart},${hunk.newLines} @@`;\\n  // Inject the changes into the function, dropping the necessary lines\\n  functionContext.splice(injectionIdx, dropCount, ...trimmedHunk.lines);\\n\\n  res.push(functionContext.join(\"\\\\n\"));\\n  res.unshift(hunkHeader);\\n  return res;\\n};\\n\\n/*\\nline nums are 0 index, file is 1 index\\n*/\\nconst combineHunks = (\\n  file: string,\\n  overlappingHunks: diff.Hunk[]\\n): diff.Hunk => {\\n  if (!overlappingHunks || overlappingHunks.length === 0) {\\n    throw \"Overlapping hunks are empty, this should never happen.\";\\n  }\\n  const sortedHunks = overlappingHunks.sort((a, b) => a.newStart - b.newStart);\\n  const fileLines = file.split(\"\\\\n\");\\n  let lastHunkEnd = sortedHunks[0].newStart + sortedHunks[0].newLines;\\n\\n  const combinedHunk: diff.Hunk = {\\n    oldStart: sortedHunks[0].oldStart,\\n    oldLines: sortedHunks[0].oldLines,\\n    newStart: sortedHunks[0].newStart,\\n    newLines: sortedHunks[0].newLines,\\n    lines: [...sortedHunks[0].lines],\\n    linedelimiters: [...sortedHunks[0].linedelimiters],\\n  };\\n\\n  for (let i = 1; i < sortedHunks.length; i++) {\\n    const hunk = sortedHunks[i];\\n\\n    // If there\\'s a gap between the last hunk and this one, add the lines in between\\n    if (hunk.newStart > lastHunkEnd) {\\n      combinedHunk.lines.push(\\n        ...fileLines.slice(lastHunkEnd - 1, hunk.newStart - 1)\\n      );\\n      combinedHunk.newLines += hunk.newStart - lastHunkEnd;\\n    }\\n\\n    combinedHunk.oldLines += hunk.oldLines;\\n    combinedHunk.newLines += hunk.newLines;\\n    combinedHunk.lines.push(...hunk.lines);\\n    combinedHunk.linedelimiters.push(...hunk.linedelimiters);\\n\\n    lastHunkEnd = hunk.newStart + hunk.newLines;\\n  }\\n  return combinedHunk;\\n};\\n\\nconst diffContextPerHunk = (file: PRFile, parser: AbstractParser) => {\\n  const updatedFile = diff.applyPatch(file.old_contents, file.patch);\\n  const patches = diff.parsePatch(file.patch);\\n  if (!updatedFile || typeof updatedFile !== \"string\") {\\n    console.log(\"APPLYING PATCH ERROR - FALLINGBACK\");\\n    throw \"THIS SHOULD NOT HAPPEN!\";\\n  }\\n\\n  const hunks: diff.Hunk[] = [];\\n  const order: number[] = [];\\n  const scopeRangeHunkMap = new Map<string, diff.Hunk[]>();\\n  const scopeRangeNodeMap = new Map<string, Node>();\\n  const expandStrategy: diff.Hunk[] = [];\\n\\n  patches.forEach((p) => {\\n    p.hunks.forEach((hunk) => {\\n      hunks.push(hunk);\\n    });\\n  });\\n\\n  hunks.forEach((hunk, idx) => {\\n    try {\\n      const trimmedHunk = trimHunk(hunk);\\n      const insertions = hunk.lines.filter((line) =>\\n        line.startsWith(\"+\")\\n      ).length;\\n      const lineStart = trimmedHunk.newStart;\\n      const lineEnd = lineStart + insertions;\\n      const largestEnclosingFunction = parser.findEnclosingContext(\\n        updatedFile,\\n        lineStart,\\n        lineEnd\\n      ).enclosingContext;\\n\\n      if (largestEnclosingFunction) {\\n        const enclosingRangeKey = `${largestEnclosingFunction.loc.start.line} -> ${largestEnclosingFunction.loc.end.line}`;\\n        let existingHunks = scopeRangeHunkMap.get(enclosingRangeKey) || [];\\n        existingHunks.push(hunk);\\n        scopeRangeHunkMap.set(enclosingRangeKey, existingHunks);\\n        scopeRangeNodeMap.set(enclosingRangeKey, largestEnclosingFunction);\\n      } else {\\n        throw \"No enclosing function.\";\\n      }\\n      order.push(idx);\\n    } catch (exc) {\\n      console.log(file.filename);\\n      console.log(\"NORMAL STRATEGY\");\\n      console.log(exc);\\n      expandStrategy.push(hunk);\\n      order.push(idx);\\n    }\\n  });\\n\\n  const scopeStategy: [string, diff.Hunk][] = []; // holds map range key and combined hunk: [[key, hunk]]\\n  for (const [range, hunks] of scopeRangeHunkMap.entries()) {\\n    const combinedHunk = combineHunks(updatedFile, hunks);\\n    scopeStategy.push([range, combinedHunk]);\\n  }\\n\\n  const contexts: string[] = [];\\n  scopeStategy.forEach(([rangeKey, hunk]) => {\\n    const context = buildingScopeString(\\n      updatedFile,\\n      scopeRangeNodeMap.get(rangeKey),\\n      hunk\\n    ).join(\"\\\\n\");\\n    contexts.push(context);\\n  });\\n  expandStrategy.forEach((hunk) => {\\n    const context = expandHunk(file.old_contents, hunk);\\n    contexts.push(context);\\n  });\\n  return contexts;\\n};\\n\\nconst functionContextPatchStrategy = (\\n  file: PRFile,\\n  parser: AbstractParser\\n): string => {\\n  let res = null;\\n  try {\\n    const contextChunks = diffContextPerHunk(file, parser);\\n    res = `## ${file.filename}\\\\n\\\\n${contextChunks.join(\"\\\\n\\\\n\")}`;\\n  } catch (exc) {\\n    console.log(exc);\\n    res = expandedPatchStrategy(file);\\n  }\\n  return res;\\n};\\n\\nexport const smarterContextPatchStrategy = (file: PRFile) => {\\n  const parser: AbstractParser = getParserForExtension(file.filename);\\n  if (parser != null) {\\n    return functionContextPatchStrategy(file, parser);\\n  } else {\\n    return expandedPatchStrategy(file);\\n  }\\n};\\n',\n",
              " 'src/constants.ts\\n\\nimport { Node } from \"@babel/traverse\";\\nimport { JavascriptParser } from \"./context/language/javascript-parser\";\\nimport { ChatCompletionMessageParam } from \"groq-sdk/resources/chat/completions\";\\n\\nexport interface PRFile {\\n  sha: string;\\n  filename: string;\\n  status:\\n    | \"added\"\\n    | \"removed\"\\n    | \"renamed\"\\n    | \"changed\"\\n    | \"modified\"\\n    | \"copied\"\\n    | \"unchanged\";\\n  additions: number;\\n  deletions: number;\\n  changes: number;\\n  blob_url: string;\\n  raw_url: string;\\n  contents_url: string;\\n  patch?: string;\\n  previous_filename?: string;\\n  patchTokenLength?: number;\\n  old_contents?: string;\\n  current_contents?: string;\\n}\\n\\nexport interface BuilderResponse {\\n  comment: string;\\n  structuredComments: any[];\\n}\\n\\nexport interface Builders {\\n  convoBuilder: (diff: string) => ChatCompletionMessageParam[];\\n  responseBuilder: (feedbacks: string[]) => Promise<BuilderResponse>;\\n}\\n\\nexport interface PatchInfo {\\n  hunks: {\\n    oldStart: number;\\n    oldLines: number;\\n    newStart: number;\\n    newLines: number;\\n    lines: string[];\\n  }[];\\n}\\n\\nexport interface PRSuggestion {\\n  describe: string;\\n  type: string;\\n  comment: string;\\n  code: string;\\n  filename: string;\\n  toString: () => string;\\n  identity: () => string;\\n}\\n\\nexport interface CodeSuggestion {\\n  file: string;\\n  line_start: number;\\n  line_end: number;\\n  correction: string;\\n  comment: string;\\n}\\n\\nexport interface ChatMessage {\\n  role: string;\\n  content: string;\\n}\\n\\nexport interface Review {\\n  review: BuilderResponse;\\n  suggestions: CodeSuggestion[];\\n}\\n\\nexport interface BranchDetails {\\n  name: string;\\n  sha: string;\\n  url: string;\\n}\\n\\nexport const sleep = async (ms: number) => {\\n  return new Promise((resolve) => setTimeout(resolve, ms));\\n};\\n\\nexport const processGitFilepath = (filepath: string) => {\\n  // Remove the leading \\'/\\' if it exists\\n  return filepath.startsWith(\"/\") ? filepath.slice(1) : filepath;\\n};\\n\\nexport interface EnclosingContext {\\n  enclosingContext: Node | null;\\n}\\n\\nexport interface AbstractParser {\\n  findEnclosingContext(\\n    file: string,\\n    lineStart: number,\\n    lineEnd: number\\n  ): EnclosingContext;\\n  dryRun(file: string): { valid: boolean; error: string };\\n}\\n\\nconst EXTENSIONS_TO_PARSERS: Map<string, AbstractParser> = new Map([\\n  [\"ts\", new JavascriptParser()],\\n  [\"tsx\", new JavascriptParser()],\\n  [\"js\", new JavascriptParser()],\\n  [\"jsx\", new JavascriptParser()],\\n]);\\n\\nexport const getParserForExtension = (filename: string) => {\\n  const fileExtension = filename.split(\".\").pop().toLowerCase();\\n  return EXTENSIONS_TO_PARSERS.get(fileExtension) || null;\\n};\\n\\nexport const assignLineNumbers = (contents: string): string => {\\n  const lines = contents.split(\"\\\\n\");\\n  let lineNumber = 1;\\n  const linesWithNumbers = lines.map((line) => {\\n    const numberedLine = `${lineNumber}: ${line}`;\\n    lineNumber++;\\n    return numberedLine;\\n  });\\n  return linesWithNumbers.join(\"\\\\n\");\\n};\\n',\n",
              " 'src/data/PRSuggestionImpl.ts\\n\\nimport { PRSuggestion } from \"../constants\";\\n\\nexport class PRSuggestionImpl implements PRSuggestion {\\n  describe: string;\\n  type: string;\\n  comment: string;\\n  code: string;\\n  filename: string;\\n\\n  constructor(\\n    describe: string,\\n    type: string,\\n    comment: string,\\n    code: string,\\n    filename: string\\n  ) {\\n    this.describe = describe;\\n    this.type = type;\\n    this.comment = comment;\\n    this.code = code;\\n    this.filename = filename;\\n  }\\n\\n  toString(): string {\\n    const xmlElements = [\\n      `<suggestion>`,\\n      `  <describe>${this.describe}</describe>`,\\n      `  <type>${this.type}</type>`,\\n      `  <comment>${this.comment}</comment>`,\\n      `  <code>${this.code}</code>`,\\n      `  <filename>${this.filename}</filename>`,\\n      `</suggestion>`,\\n    ];\\n    return xmlElements.join(\"\\\\n\");\\n  }\\n\\n  identity(): string {\\n    return `${this.filename}:${this.comment}`;\\n  }\\n}\\n']"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "augmented_query = \"\\n\" + \"\\n\\n-------\\n\\n\".join(context) + \"\\n-------\\n\\n\\n\\n\\nMY QUESTION:\\n\" + query\n"
      ],
      "metadata": {
        "id": "FBk9KXiucSD3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(augmented_query)\n"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8obtLlVucTkl",
        "outputId": "ef98f306-2e38-4dfb-d3e5-df3a26ea8160"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "src/context/language/javascript-parser.ts\n",
            "\n",
            "import { AbstractParser, EnclosingContext } from \"../../constants\";\n",
            "import * as parser from \"@babel/parser\";\n",
            "import traverse, { NodePath, Node } from \"@babel/traverse\";\n",
            "\n",
            "const processNode = (\n",
            "  path: NodePath<Node>,\n",
            "  lineStart: number,\n",
            "  lineEnd: number,\n",
            "  largestSize: number,\n",
            "  largestEnclosingContext: Node | null\n",
            ") => {\n",
            "  const { start, end } = path.node.loc;\n",
            "  if (start.line <= lineStart && lineEnd <= end.line) {\n",
            "    const size = end.line - start.line;\n",
            "    if (size > largestSize) {\n",
            "      largestSize = size;\n",
            "      largestEnclosingContext = path.node;\n",
            "    }\n",
            "  }\n",
            "  return { largestSize, largestEnclosingContext };\n",
            "};\n",
            "\n",
            "export class JavascriptParser implements AbstractParser {\n",
            "  findEnclosingContext(\n",
            "    file: string,\n",
            "    lineStart: number,\n",
            "    lineEnd: number\n",
            "  ): EnclosingContext {\n",
            "    const ast = parser.parse(file, {\n",
            "      sourceType: \"module\",\n",
            "      plugins: [\"jsx\", \"typescript\"], // To allow JSX and TypeScript\n",
            "    });\n",
            "    let largestEnclosingContext: Node = null;\n",
            "    let largestSize = 0;\n",
            "    traverse(ast, {\n",
            "      Function(path) {\n",
            "        ({ largestSize, largestEnclosingContext } = processNode(\n",
            "          path,\n",
            "          lineStart,\n",
            "          lineEnd,\n",
            "          largestSize,\n",
            "          largestEnclosingContext\n",
            "        ));\n",
            "      },\n",
            "      TSInterfaceDeclaration(path) {\n",
            "        ({ largestSize, largestEnclosingContext } = processNode(\n",
            "          path,\n",
            "          lineStart,\n",
            "          lineEnd,\n",
            "          largestSize,\n",
            "          largestEnclosingContext\n",
            "        ));\n",
            "      },\n",
            "    });\n",
            "    return {\n",
            "      enclosingContext: largestEnclosingContext,\n",
            "    } as EnclosingContext;\n",
            "  }\n",
            "\n",
            "  dryRun(file: string): { valid: boolean; error: string } {\n",
            "    try {\n",
            "      const ast = parser.parse(file, {\n",
            "        sourceType: \"module\",\n",
            "        plugins: [\"jsx\", \"typescript\"], // To allow JSX and TypeScript\n",
            "      });\n",
            "      return {\n",
            "        valid: true,\n",
            "        error: \"\",\n",
            "      };\n",
            "    } catch (exc) {\n",
            "      return {\n",
            "        valid: false,\n",
            "        error: exc,\n",
            "      };\n",
            "    }\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "-------\n",
            "\n",
            "src/context/language/python-parser.ts\n",
            "\n",
            "import { AbstractParser, EnclosingContext } from \"../../constants\";\n",
            "export class PythonParser implements AbstractParser {\n",
            "  findEnclosingContext(\n",
            "    file: string,\n",
            "    lineStart: number,\n",
            "    lineEnd: number\n",
            "  ): EnclosingContext {\n",
            "    // TODO: Implement this method for Python\n",
            "    return null;\n",
            "  }\n",
            "  dryRun(file: string): { valid: boolean; error: string } {\n",
            "    // TODO: Implement this method for Python\n",
            "    return { valid: false, error: \"Not implemented yet\" };\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "-------\n",
            "\n",
            "src/context/review.ts\n",
            "\n",
            "import {\n",
            "  AbstractParser,\n",
            "  PRFile,\n",
            "  PatchInfo,\n",
            "  getParserForExtension,\n",
            "} from \"../constants\";\n",
            "import * as diff from \"diff\";\n",
            "import { JavascriptParser } from \"./language/javascript-parser\";\n",
            "import { Node } from \"@babel/traverse\";\n",
            "\n",
            "const expandHunk = (\n",
            "  contents: string,\n",
            "  hunk: diff.Hunk,\n",
            "  linesAbove: number = 5,\n",
            "  linesBelow: number = 5\n",
            ") => {\n",
            "  const fileLines = contents.split(\"\\n\");\n",
            "  const curExpansion: string[] = [];\n",
            "  const start = Math.max(0, hunk.oldStart - 1 - linesAbove);\n",
            "  const end = Math.min(\n",
            "    fileLines.length,\n",
            "    hunk.oldStart - 1 + hunk.oldLines + linesBelow\n",
            "  );\n",
            "\n",
            "  for (let i = start; i < hunk.oldStart - 1; i++) {\n",
            "    curExpansion.push(fileLines[i]);\n",
            "  }\n",
            "\n",
            "  curExpansion.push(\n",
            "    `@@ -${hunk.oldStart},${hunk.oldLines} +${hunk.newStart},${hunk.newLines} @@`\n",
            "  );\n",
            "  hunk.lines.forEach((line) => {\n",
            "    if (!curExpansion.includes(line)) {\n",
            "      curExpansion.push(line);\n",
            "    }\n",
            "  });\n",
            "\n",
            "  for (let i = hunk.oldStart - 1 + hunk.oldLines; i < end; i++) {\n",
            "    curExpansion.push(fileLines[i]);\n",
            "  }\n",
            "  return curExpansion.join(\"\\n\");\n",
            "};\n",
            "\n",
            "const expandFileLines = (\n",
            "  file: PRFile,\n",
            "  linesAbove: number = 5,\n",
            "  linesBelow: number = 5\n",
            ") => {\n",
            "  const fileLines = file.old_contents.split(\"\\n\");\n",
            "  const patches: PatchInfo[] = diff.parsePatch(file.patch);\n",
            "  const expandedLines: string[][] = [];\n",
            "  patches.forEach((patch) => {\n",
            "    patch.hunks.forEach((hunk) => {\n",
            "      const curExpansion: string[] = [];\n",
            "      const start = Math.max(0, hunk.oldStart - 1 - linesAbove);\n",
            "      const end = Math.min(\n",
            "        fileLines.length,\n",
            "        hunk.oldStart - 1 + hunk.oldLines + linesBelow\n",
            "      );\n",
            "\n",
            "      for (let i = start; i < hunk.oldStart - 1; i++) {\n",
            "        curExpansion.push(fileLines[i]);\n",
            "      }\n",
            "\n",
            "      curExpansion.push(\n",
            "        `@@ -${hunk.oldStart},${hunk.oldLines} +${hunk.newStart},${hunk.newLines} @@`\n",
            "      );\n",
            "      hunk.lines.forEach((line) => {\n",
            "        if (!curExpansion.includes(line)) {\n",
            "          curExpansion.push(line);\n",
            "        }\n",
            "      });\n",
            "\n",
            "      for (let i = hunk.oldStart - 1 + hunk.oldLines; i < end; i++) {\n",
            "        curExpansion.push(fileLines[i]);\n",
            "      }\n",
            "      expandedLines.push(curExpansion);\n",
            "    });\n",
            "  });\n",
            "\n",
            "  return expandedLines;\n",
            "};\n",
            "\n",
            "export const expandedPatchStrategy = (file: PRFile) => {\n",
            "  const expandedPatches = expandFileLines(file);\n",
            "  const expansions = expandedPatches\n",
            "    .map((patchLines) => patchLines.join(\"\\n\"))\n",
            "    .join(\"\\n\\n\");\n",
            "  return `## ${file.filename}\\n\\n${expansions}`;\n",
            "};\n",
            "\n",
            "export const rawPatchStrategy = (file: PRFile) => {\n",
            "  return `## ${file.filename}\\n\\n${file.patch}`;\n",
            "};\n",
            "\n",
            "const trimHunk = (hunk: diff.Hunk): diff.Hunk => {\n",
            "  const startIdx = hunk.lines.findIndex(\n",
            "    (line) => line.startsWith(\"+\") || line.startsWith(\"-\")\n",
            "  );\n",
            "  const endIdx = hunk.lines\n",
            "    .slice()\n",
            "    .reverse()\n",
            "    .findIndex((line) => line.startsWith(\"+\") || line.startsWith(\"-\"));\n",
            "  const editLines = hunk.lines.slice(startIdx, hunk.lines.length - endIdx);\n",
            "  return { ...hunk, lines: editLines, newStart: startIdx + hunk.newStart };\n",
            "};\n",
            "\n",
            "const buildingScopeString = (\n",
            "  currentFile: string,\n",
            "  scope: Node,\n",
            "  hunk: diff.Hunk\n",
            ") => {\n",
            "  const res: string[] = [];\n",
            "  const trimmedHunk = trimHunk(hunk);\n",
            "  const functionStartLine = scope.loc.start.line;\n",
            "  const functionEndLine = scope.loc.end.line;\n",
            "  const updatedFileLines = currentFile.split(\"\\n\");\n",
            "  // Extract the lines of the function\n",
            "  const functionContext = updatedFileLines.slice(\n",
            "    functionStartLine - 1,\n",
            "    functionEndLine\n",
            "  );\n",
            "  // Calculate the index where the changes should be injected into the function\n",
            "  const injectionIdx =\n",
            "    hunk.newStart -\n",
            "    functionStartLine +\n",
            "    hunk.lines.findIndex(\n",
            "      (line) => line.startsWith(\"+\") || line.startsWith(\"-\")\n",
            "    );\n",
            "  // Count the number of lines that should be dropped from the function\n",
            "  const dropCount = trimmedHunk.lines.filter(\n",
            "    (line) => !line.startsWith(\"-\")\n",
            "  ).length;\n",
            "\n",
            "  const hunkHeader = `@@ -${hunk.oldStart},${hunk.oldLines} +${hunk.newStart},${hunk.newLines} @@`;\n",
            "  // Inject the changes into the function, dropping the necessary lines\n",
            "  functionContext.splice(injectionIdx, dropCount, ...trimmedHunk.lines);\n",
            "\n",
            "  res.push(functionContext.join(\"\\n\"));\n",
            "  res.unshift(hunkHeader);\n",
            "  return res;\n",
            "};\n",
            "\n",
            "/*\n",
            "line nums are 0 index, file is 1 index\n",
            "*/\n",
            "const combineHunks = (\n",
            "  file: string,\n",
            "  overlappingHunks: diff.Hunk[]\n",
            "): diff.Hunk => {\n",
            "  if (!overlappingHunks || overlappingHunks.length === 0) {\n",
            "    throw \"Overlapping hunks are empty, this should never happen.\";\n",
            "  }\n",
            "  const sortedHunks = overlappingHunks.sort((a, b) => a.newStart - b.newStart);\n",
            "  const fileLines = file.split(\"\\n\");\n",
            "  let lastHunkEnd = sortedHunks[0].newStart + sortedHunks[0].newLines;\n",
            "\n",
            "  const combinedHunk: diff.Hunk = {\n",
            "    oldStart: sortedHunks[0].oldStart,\n",
            "    oldLines: sortedHunks[0].oldLines,\n",
            "    newStart: sortedHunks[0].newStart,\n",
            "    newLines: sortedHunks[0].newLines,\n",
            "    lines: [...sortedHunks[0].lines],\n",
            "    linedelimiters: [...sortedHunks[0].linedelimiters],\n",
            "  };\n",
            "\n",
            "  for (let i = 1; i < sortedHunks.length; i++) {\n",
            "    const hunk = sortedHunks[i];\n",
            "\n",
            "    // If there's a gap between the last hunk and this one, add the lines in between\n",
            "    if (hunk.newStart > lastHunkEnd) {\n",
            "      combinedHunk.lines.push(\n",
            "        ...fileLines.slice(lastHunkEnd - 1, hunk.newStart - 1)\n",
            "      );\n",
            "      combinedHunk.newLines += hunk.newStart - lastHunkEnd;\n",
            "    }\n",
            "\n",
            "    combinedHunk.oldLines += hunk.oldLines;\n",
            "    combinedHunk.newLines += hunk.newLines;\n",
            "    combinedHunk.lines.push(...hunk.lines);\n",
            "    combinedHunk.linedelimiters.push(...hunk.linedelimiters);\n",
            "\n",
            "    lastHunkEnd = hunk.newStart + hunk.newLines;\n",
            "  }\n",
            "  return combinedHunk;\n",
            "};\n",
            "\n",
            "const diffContextPerHunk = (file: PRFile, parser: AbstractParser) => {\n",
            "  const updatedFile = diff.applyPatch(file.old_contents, file.patch);\n",
            "  const patches = diff.parsePatch(file.patch);\n",
            "  if (!updatedFile || typeof updatedFile !== \"string\") {\n",
            "    console.log(\"APPLYING PATCH ERROR - FALLINGBACK\");\n",
            "    throw \"THIS SHOULD NOT HAPPEN!\";\n",
            "  }\n",
            "\n",
            "  const hunks: diff.Hunk[] = [];\n",
            "  const order: number[] = [];\n",
            "  const scopeRangeHunkMap = new Map<string, diff.Hunk[]>();\n",
            "  const scopeRangeNodeMap = new Map<string, Node>();\n",
            "  const expandStrategy: diff.Hunk[] = [];\n",
            "\n",
            "  patches.forEach((p) => {\n",
            "    p.hunks.forEach((hunk) => {\n",
            "      hunks.push(hunk);\n",
            "    });\n",
            "  });\n",
            "\n",
            "  hunks.forEach((hunk, idx) => {\n",
            "    try {\n",
            "      const trimmedHunk = trimHunk(hunk);\n",
            "      const insertions = hunk.lines.filter((line) =>\n",
            "        line.startsWith(\"+\")\n",
            "      ).length;\n",
            "      const lineStart = trimmedHunk.newStart;\n",
            "      const lineEnd = lineStart + insertions;\n",
            "      const largestEnclosingFunction = parser.findEnclosingContext(\n",
            "        updatedFile,\n",
            "        lineStart,\n",
            "        lineEnd\n",
            "      ).enclosingContext;\n",
            "\n",
            "      if (largestEnclosingFunction) {\n",
            "        const enclosingRangeKey = `${largestEnclosingFunction.loc.start.line} -> ${largestEnclosingFunction.loc.end.line}`;\n",
            "        let existingHunks = scopeRangeHunkMap.get(enclosingRangeKey) || [];\n",
            "        existingHunks.push(hunk);\n",
            "        scopeRangeHunkMap.set(enclosingRangeKey, existingHunks);\n",
            "        scopeRangeNodeMap.set(enclosingRangeKey, largestEnclosingFunction);\n",
            "      } else {\n",
            "        throw \"No enclosing function.\";\n",
            "      }\n",
            "      order.push(idx);\n",
            "    } catch (exc) {\n",
            "      console.log(file.filename);\n",
            "      console.log(\"NORMAL STRATEGY\");\n",
            "      console.log(exc);\n",
            "      expandStrategy.push(hunk);\n",
            "      order.push(idx);\n",
            "    }\n",
            "  });\n",
            "\n",
            "  const scopeStategy: [string, diff.Hunk][] = []; // holds map range key and combined hunk: [[key, hunk]]\n",
            "  for (const [range, hunks] of scopeRangeHunkMap.entries()) {\n",
            "    const combinedHunk = combineHunks(updatedFile, hunks);\n",
            "    scopeStategy.push([range, combinedHunk]);\n",
            "  }\n",
            "\n",
            "  const contexts: string[] = [];\n",
            "  scopeStategy.forEach(([rangeKey, hunk]) => {\n",
            "    const context = buildingScopeString(\n",
            "      updatedFile,\n",
            "      scopeRangeNodeMap.get(rangeKey),\n",
            "      hunk\n",
            "    ).join(\"\\n\");\n",
            "    contexts.push(context);\n",
            "  });\n",
            "  expandStrategy.forEach((hunk) => {\n",
            "    const context = expandHunk(file.old_contents, hunk);\n",
            "    contexts.push(context);\n",
            "  });\n",
            "  return contexts;\n",
            "};\n",
            "\n",
            "const functionContextPatchStrategy = (\n",
            "  file: PRFile,\n",
            "  parser: AbstractParser\n",
            "): string => {\n",
            "  let res = null;\n",
            "  try {\n",
            "    const contextChunks = diffContextPerHunk(file, parser);\n",
            "    res = `## ${file.filename}\\n\\n${contextChunks.join(\"\\n\\n\")}`;\n",
            "  } catch (exc) {\n",
            "    console.log(exc);\n",
            "    res = expandedPatchStrategy(file);\n",
            "  }\n",
            "  return res;\n",
            "};\n",
            "\n",
            "export const smarterContextPatchStrategy = (file: PRFile) => {\n",
            "  const parser: AbstractParser = getParserForExtension(file.filename);\n",
            "  if (parser != null) {\n",
            "    return functionContextPatchStrategy(file, parser);\n",
            "  } else {\n",
            "    return expandedPatchStrategy(file);\n",
            "  }\n",
            "};\n",
            "\n",
            "\n",
            "-------\n",
            "\n",
            "src/constants.ts\n",
            "\n",
            "import { Node } from \"@babel/traverse\";\n",
            "import { JavascriptParser } from \"./context/language/javascript-parser\";\n",
            "import { ChatCompletionMessageParam } from \"groq-sdk/resources/chat/completions\";\n",
            "\n",
            "export interface PRFile {\n",
            "  sha: string;\n",
            "  filename: string;\n",
            "  status:\n",
            "    | \"added\"\n",
            "    | \"removed\"\n",
            "    | \"renamed\"\n",
            "    | \"changed\"\n",
            "    | \"modified\"\n",
            "    | \"copied\"\n",
            "    | \"unchanged\";\n",
            "  additions: number;\n",
            "  deletions: number;\n",
            "  changes: number;\n",
            "  blob_url: string;\n",
            "  raw_url: string;\n",
            "  contents_url: string;\n",
            "  patch?: string;\n",
            "  previous_filename?: string;\n",
            "  patchTokenLength?: number;\n",
            "  old_contents?: string;\n",
            "  current_contents?: string;\n",
            "}\n",
            "\n",
            "export interface BuilderResponse {\n",
            "  comment: string;\n",
            "  structuredComments: any[];\n",
            "}\n",
            "\n",
            "export interface Builders {\n",
            "  convoBuilder: (diff: string) => ChatCompletionMessageParam[];\n",
            "  responseBuilder: (feedbacks: string[]) => Promise<BuilderResponse>;\n",
            "}\n",
            "\n",
            "export interface PatchInfo {\n",
            "  hunks: {\n",
            "    oldStart: number;\n",
            "    oldLines: number;\n",
            "    newStart: number;\n",
            "    newLines: number;\n",
            "    lines: string[];\n",
            "  }[];\n",
            "}\n",
            "\n",
            "export interface PRSuggestion {\n",
            "  describe: string;\n",
            "  type: string;\n",
            "  comment: string;\n",
            "  code: string;\n",
            "  filename: string;\n",
            "  toString: () => string;\n",
            "  identity: () => string;\n",
            "}\n",
            "\n",
            "export interface CodeSuggestion {\n",
            "  file: string;\n",
            "  line_start: number;\n",
            "  line_end: number;\n",
            "  correction: string;\n",
            "  comment: string;\n",
            "}\n",
            "\n",
            "export interface ChatMessage {\n",
            "  role: string;\n",
            "  content: string;\n",
            "}\n",
            "\n",
            "export interface Review {\n",
            "  review: BuilderResponse;\n",
            "  suggestions: CodeSuggestion[];\n",
            "}\n",
            "\n",
            "export interface BranchDetails {\n",
            "  name: string;\n",
            "  sha: string;\n",
            "  url: string;\n",
            "}\n",
            "\n",
            "export const sleep = async (ms: number) => {\n",
            "  return new Promise((resolve) => setTimeout(resolve, ms));\n",
            "};\n",
            "\n",
            "export const processGitFilepath = (filepath: string) => {\n",
            "  // Remove the leading '/' if it exists\n",
            "  return filepath.startsWith(\"/\") ? filepath.slice(1) : filepath;\n",
            "};\n",
            "\n",
            "export interface EnclosingContext {\n",
            "  enclosingContext: Node | null;\n",
            "}\n",
            "\n",
            "export interface AbstractParser {\n",
            "  findEnclosingContext(\n",
            "    file: string,\n",
            "    lineStart: number,\n",
            "    lineEnd: number\n",
            "  ): EnclosingContext;\n",
            "  dryRun(file: string): { valid: boolean; error: string };\n",
            "}\n",
            "\n",
            "const EXTENSIONS_TO_PARSERS: Map<string, AbstractParser> = new Map([\n",
            "  [\"ts\", new JavascriptParser()],\n",
            "  [\"tsx\", new JavascriptParser()],\n",
            "  [\"js\", new JavascriptParser()],\n",
            "  [\"jsx\", new JavascriptParser()],\n",
            "]);\n",
            "\n",
            "export const getParserForExtension = (filename: string) => {\n",
            "  const fileExtension = filename.split(\".\").pop().toLowerCase();\n",
            "  return EXTENSIONS_TO_PARSERS.get(fileExtension) || null;\n",
            "};\n",
            "\n",
            "export const assignLineNumbers = (contents: string): string => {\n",
            "  const lines = contents.split(\"\\n\");\n",
            "  let lineNumber = 1;\n",
            "  const linesWithNumbers = lines.map((line) => {\n",
            "    const numberedLine = `${lineNumber}: ${line}`;\n",
            "    lineNumber++;\n",
            "    return numberedLine;\n",
            "  });\n",
            "  return linesWithNumbers.join(\"\\n\");\n",
            "};\n",
            "\n",
            "\n",
            "-------\n",
            "\n",
            "src/data/PRSuggestionImpl.ts\n",
            "\n",
            "import { PRSuggestion } from \"../constants\";\n",
            "\n",
            "export class PRSuggestionImpl implements PRSuggestion {\n",
            "  describe: string;\n",
            "  type: string;\n",
            "  comment: string;\n",
            "  code: string;\n",
            "  filename: string;\n",
            "\n",
            "  constructor(\n",
            "    describe: string,\n",
            "    type: string,\n",
            "    comment: string,\n",
            "    code: string,\n",
            "    filename: string\n",
            "  ) {\n",
            "    this.describe = describe;\n",
            "    this.type = type;\n",
            "    this.comment = comment;\n",
            "    this.code = code;\n",
            "    this.filename = filename;\n",
            "  }\n",
            "\n",
            "  toString(): string {\n",
            "    const xmlElements = [\n",
            "      `<suggestion>`,\n",
            "      `  <describe>${this.describe}</describe>`,\n",
            "      `  <type>${this.type}</type>`,\n",
            "      `  <comment>${this.comment}</comment>`,\n",
            "      `  <code>${this.code}</code>`,\n",
            "      `  <filename>${this.filename}</filename>`,\n",
            "      `</suggestion>`,\n",
            "    ];\n",
            "    return xmlElements.join(\"\\n\");\n",
            "  }\n",
            "\n",
            "  identity(): string {\n",
            "    return `${this.filename}:${this.comment}`;\n",
            "  }\n",
            "}\n",
            "\n",
            "-------\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "MY QUESTION:\n",
            "How is the javascript parser used?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "system_prompt = \"\"\"You are a Senior Software Engineer, who is an expert in TypeScript.\n",
        "\n",
        "Answer the question I have about the codebase based on the context provided. Always consider all of the context provided\n",
        "to answer my question.\n",
        "\"\"\"\n",
        "\n",
        "llm_response = client.chat.completions.create(\n",
        "    model=\"deepseek/deepseek-r1:free\",\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": system_prompt},\n",
        "        {\"role\": \"user\", \"content\": augmented_query}\n",
        "    ]\n",
        ")\n",
        "\n",
        "response = llm_response.choices[0].message.content"
      ],
      "metadata": {
        "id": "ShP-fVHGcXfa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(response)"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i9O_0ojGcbFI",
        "outputId": "ecf9c983-a2a1-4c31-840c-967da9784486"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Flask framework is used in this codebase to create a web application with the following key functionalities:\n",
            "\n",
            "1. **Basic Setup**:\n",
            "- Initialized with `Flask(__name__)`\n",
            "- Uses environment variables with `python-dotenv`\n",
            "- Runs with `app.run()` using configurable host/port\n",
            "\n",
            "2. **Routes**:\n",
            "- `/`: Serves home page template (index.html)\n",
            "- `/chatbot`: Serves chatbot interface template (chatbot.html)\n",
            "- `/submit_media`: POST endpoint for processing media/documents\n",
            "- `/ask_question`: POST endpoint for handling user queries\n",
            "\n",
            "3. **Key Flask Features Used**:\n",
            "- `request` object: Handling form data and file uploads\n",
            "- `jsonify`: Returning JSON responses for API endpoints\n",
            "- `render_template`: Serving HTML templates\n",
            "- Route decorators (`@app.route`) for URL routing\n",
            "- Form data handling with `request.form.getlist()`\n",
            "- File upload handling with `request.files.getlist()`\n",
            "\n",
            "4. **Error Handling**:\n",
            "- Try/except blocks in routes\n",
            "- HTTP status codes (400, 500)\n",
            "- JSON error responses with error details\n",
            "\n",
            "5. **Integration Pattern**:\n",
            "- Centralized initialization of ML/NLP components (GroqClass, DocumentProcessor)\n",
            "- Directory management for uploaded files\n",
            "- Integration with Pinecone vector database\n",
            "- YouTube transcription workflow\n",
            "\n",
            "The application follows a typical Flask pattern of route handling with business logic separated into support classes (GroqClass, DocumentProcessor) and uses Flask's built-in capabilities for web request handling and response generation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def perform_rag(query, model=\"deepseek/deepseek-r1:free\"):\n",
        "    raw_query_embedding = get_huggingface_embeddings(query)\n",
        "\n",
        "    top_matches = pinecone_index.query(vector=raw_query_embedding.tolist(), top_k=5, include_metadata=True, namespace=\"https://github.com/CoderAgent/SecureAgent\")\n",
        "\n",
        "    # Get the list of retrieved texts\n",
        "    contexts = [item['metadata']['text'] for item in top_matches['matches']]\n",
        "\n",
        "    augmented_query = \"\\n\" + \"\\n\\n-------\\n\\n\".join(contexts[ : 10]) + \"\\n-------\\n\\n\\n\\n\\nMY QUESTION:\\n\" + query\n",
        "\n",
        "    # Modify the prompt below as need to improve the response quality\n",
        "    system_prompt = f\"\"\"You are a Senior Software Engineer, specializing in TypeScript.\n",
        "\n",
        "    Answer any questions I have about the codebase, based on the code provided. Always consider all of the context provided when forming a response.\n",
        "    \"\"\"\n",
        "\n",
        "    llm_response = client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": system_prompt},\n",
        "            {\"role\": \"user\", \"content\": augmented_query}\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    return llm_response.choices[0].message.content"
      ],
      "metadata": {
        "id": "ci_suK7IcewK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = perform_rag(\"What does this github repo do?\")\n",
        "\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n2U5MHY5cg5u",
        "outputId": "7f3b58c8-5f0d-4667-c7fa-2dac5a28edad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This GitHub repo contains a full-stack application that serves as an interface for OpenAI's text generation API. Here's a detailed breakdown of its functionality:\n",
            "\n",
            "**Core Features:**\n",
            "1. **AI Text Generation:**\n",
            "   - Users can input prompts and generate text completions using OpenAI's GPT models\n",
            "   - Supports adjustable parameters:\n",
            "     - Temperature (creativity control)\n",
            "     - Max length (response size)\n",
            "\n",
            "2. **Model Management:**\n",
            "   - Fetches available OpenAI models\n",
            "   - Built-in model configuration system\n",
            "\n",
            "3. **Context Management:**\n",
            "   - Central state management using React Context API\n",
            "   - Tracks generation history\n",
            "   - Manages model settings across components\n",
            "\n",
            "**Technical Architecture:**\n",
            "- **Frontend** (Next.js/React):\n",
            "  - Context providers for state management\n",
            "  - Custom hooks for API interactions\n",
            "  - Interactive UI with:\n",
            "    - Settings controls\n",
            "    - Prompt input\n",
            "    - History display\n",
            "    - Error handling\n",
            "\n",
            "- **Backend** (Next.js API routes):\n",
            "  - Proxy server handling secure OpenAI API communication\n",
            "  - Two endpoints:\n",
            "    1. `/api/generate` - Handles text generation\n",
            "    2. `/api/models` - Retrieves available models\n",
            "\n",
            "- **Security:**\n",
            "  - Keeps OpenAI API keys server-side\n",
            "  - Prevents client-side exposure of sensitive credentials\n",
            "\n",
            "**Key Components:**\n",
            "1. `ModelProvider` - Main context manager\n",
            "2. `modelAPI.ts` - API client for OpenAI calls\n",
            "3. `pages/api/*` - Secure backend endpoints\n",
            "4. Interactive UI components for:\n",
            "   - Settings adjustment\n",
            "   - Prompt input\n",
            "   - Generated results display\n",
            "   - Historical interaction tracking\n",
            "\n",
            "**Development Features:**\n",
            "- TypeScript implementation\n",
            "- Unit tests for core functionality\n",
            "- Context-based state pattern\n",
            "- Error handling and loading states\n",
            "\n",
            "This stack allows users to safely interact with OpenAI's API through a client-friendly interface while maintaining security best practices through the Next.js backend proxy layer.\n"
          ]
        }
      ]
    }
  ]
}